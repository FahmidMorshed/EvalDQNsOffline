{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-23T22:50:47.731949Z",
     "start_time": "2020-10-23T22:50:47.461438Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cartpole | Total transitions: 116155  | Total episodes: 5001\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_id</th>\n",
       "      <th>transition_id</th>\n",
       "      <th>current state</th>\n",
       "      <th>action</th>\n",
       "      <th>reward</th>\n",
       "      <th>delayed_reward</th>\n",
       "      <th>done</th>\n",
       "      <th>next_state</th>\n",
       "      <th>info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.04257099 -0.02952321  0.01411527 -0.03864165]</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>[-0.04316146  0.16539351  0.01334243 -0.32683786]</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[-0.04316146  0.16539351  0.01334243 -0.32683786]</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>[-0.03985359 -0.02991583  0.00680568 -0.02997736]</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[-0.03985359 -0.02991583  0.00680568 -0.02997736]</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>[-0.0404519   0.16510786  0.00620613 -0.32050528]</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>[-0.0404519   0.16510786  0.00620613 -0.32050528]</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>[-3.71497462e-02  3.60140882e-01 -2.03977347e-...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>[-3.71497462e-02  3.60140882e-01 -2.03977347e-...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>[-0.02994693  0.55526568 -0.01242847 -0.90397175]</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116150</th>\n",
       "      <td>5001</td>\n",
       "      <td>19</td>\n",
       "      <td>[ 0.0678786   0.05469085 -0.17322402 -0.5183969 ]</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>[ 0.06897241 -0.13762336 -0.18359195 -0.28491633]</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116151</th>\n",
       "      <td>5001</td>\n",
       "      <td>20</td>\n",
       "      <td>[ 0.06897241 -0.13762336 -0.18359195 -0.28491633]</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>[ 0.06621995  0.05957742 -0.18929028 -0.62941968]</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116152</th>\n",
       "      <td>5001</td>\n",
       "      <td>21</td>\n",
       "      <td>[ 0.06621995  0.05957742 -0.18929028 -0.62941968]</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>[ 0.06741149 -0.13246942 -0.20187867 -0.40181452]</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116153</th>\n",
       "      <td>5001</td>\n",
       "      <td>22</td>\n",
       "      <td>[ 0.06741149 -0.13246942 -0.20187867 -0.40181452]</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>[ 0.06476211 -0.32424159 -0.20991496 -0.17894814]</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116154</th>\n",
       "      <td>5001</td>\n",
       "      <td>22</td>\n",
       "      <td>[ 0.06476211 -0.32424159 -0.20991496 -0.17894814]</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>True</td>\n",
       "      <td>[ 0.06476211 -0.32424159 -0.20991496 -0.17894814]</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>116155 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        episode_id  transition_id  \\\n",
       "0                1              1   \n",
       "1                1              2   \n",
       "2                1              3   \n",
       "3                1              4   \n",
       "4                1              5   \n",
       "...            ...            ...   \n",
       "116150        5001             19   \n",
       "116151        5001             20   \n",
       "116152        5001             21   \n",
       "116153        5001             22   \n",
       "116154        5001             22   \n",
       "\n",
       "                                            current state  action  reward  \\\n",
       "0       [-0.04257099 -0.02952321  0.01411527 -0.03864165]       1     1.0   \n",
       "1       [-0.04316146  0.16539351  0.01334243 -0.32683786]       0     1.0   \n",
       "2       [-0.03985359 -0.02991583  0.00680568 -0.02997736]       1     1.0   \n",
       "3       [-0.0404519   0.16510786  0.00620613 -0.32050528]       1     1.0   \n",
       "4       [-3.71497462e-02  3.60140882e-01 -2.03977347e-...       1     1.0   \n",
       "...                                                   ...     ...     ...   \n",
       "116150  [ 0.0678786   0.05469085 -0.17322402 -0.5183969 ]       0     1.0   \n",
       "116151  [ 0.06897241 -0.13762336 -0.18359195 -0.28491633]       1     1.0   \n",
       "116152  [ 0.06621995  0.05957742 -0.18929028 -0.62941968]       0     1.0   \n",
       "116153  [ 0.06741149 -0.13246942 -0.20187867 -0.40181452]       0    -1.0   \n",
       "116154  [ 0.06476211 -0.32424159 -0.20991496 -0.17894814]       0    -1.0   \n",
       "\n",
       "        delayed_reward   done  \\\n",
       "0                  0.0  False   \n",
       "1                  0.0  False   \n",
       "2                  0.0  False   \n",
       "3                  0.0  False   \n",
       "4                  0.0  False   \n",
       "...                ...    ...   \n",
       "116150             0.0  False   \n",
       "116151             0.0  False   \n",
       "116152             0.0  False   \n",
       "116153             0.0   True   \n",
       "116154            20.0   True   \n",
       "\n",
       "                                               next_state info  \n",
       "0       [-0.04316146  0.16539351  0.01334243 -0.32683786]   {}  \n",
       "1       [-0.03985359 -0.02991583  0.00680568 -0.02997736]   {}  \n",
       "2       [-0.0404519   0.16510786  0.00620613 -0.32050528]   {}  \n",
       "3       [-3.71497462e-02  3.60140882e-01 -2.03977347e-...   {}  \n",
       "4       [-0.02994693  0.55526568 -0.01242847 -0.90397175]   {}  \n",
       "...                                                   ...  ...  \n",
       "116150  [ 0.06897241 -0.13762336 -0.18359195 -0.28491633]   {}  \n",
       "116151  [ 0.06621995  0.05957742 -0.18929028 -0.62941968]   {}  \n",
       "116152  [ 0.06741149 -0.13246942 -0.20187867 -0.40181452]   {}  \n",
       "116153  [ 0.06476211 -0.32424159 -0.20991496 -0.17894814]   {}  \n",
       "116154  [ 0.06476211 -0.32424159 -0.20991496 -0.17894814]   {}  \n",
       "\n",
       "[116155 rows x 9 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import pdb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "import keras.layers as layers\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "\n",
    "random_state=0\n",
    "np.random.seed(random_state)\n",
    "random.seed(random_state)\n",
    "\n",
    "df = pd.read_csv('../data/CartPole-v1_5k.csv')\n",
    "print(\"Cartpole\", \"| Total transitions:\", len(df), \" | Total episodes:\", len(df['episode_id'].unique()))\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-23T23:06:30.759235Z",
     "start_time": "2020-10-23T23:06:30.725511Z"
    },
    "code_folding": [
     73,
     88
    ]
   },
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, df_batch, state_size, action_size, \n",
    "                 minibatch_size=32, gamma=.9, lr=0.0001, units=128,\n",
    "                 dueling=False, double_param=0, priority_aplha = 0,\n",
    "                 copy_online_to_target_ep=100, eval_after=100):\n",
    "        \n",
    "        # setting parameters\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.batch = df_batch\n",
    "        \n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.gamma = gamma\n",
    "        self.learning_rate = lr\n",
    "        self.units = units\n",
    "        \n",
    "        self.dueling = dueling\n",
    "        self.double_param = double_param\n",
    "        self.priority_aplha = priority_aplha\n",
    "        \n",
    "        self.copy_online_to_target_ep = copy_online_to_target_ep\n",
    "        self.eval_after = eval_after\n",
    "        \n",
    "        \n",
    "        # setting up the models\n",
    "        if self.dueling:\n",
    "            # TODO\n",
    "            self.model_1 = self._build_model_dueling()\n",
    "            self.model_2 = self._build_model_dueling()\n",
    "        else:\n",
    "            self.model_1 = self._build_model()\n",
    "            self.model_2 = self._build_model()\n",
    "        \n",
    "        # evaluation variables\n",
    "        self.R = []\n",
    "        self.ecrs = []\n",
    "        \n",
    "    def _build_model(self):\n",
    "        \"\"\"\n",
    "        Standard DQN model\n",
    "        \"\"\"\n",
    "        model = Sequential()\n",
    "        \n",
    "        \n",
    "        model.add(layers.Dense(self.units, input_dim=self.state_size, activation='relu', kernel_initializer='glorot_normal'))\n",
    "        model.add(layers.Dense(self.units, activation='relu', kernel_initializer='glorot_normal'))\n",
    "        \n",
    "        model.add(layers.Dense(self.action_size, activation='linear', kernel_initializer='glorot_normal'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate), metrics=[tf.keras.metrics.RootMeanSquaredError(), 'mae'])\n",
    "        return model\n",
    "    \n",
    "    def learn(self, epoch, env=None):\n",
    "        for i in range(epoch):\n",
    "            self._learn_minibatch()\n",
    "            \n",
    "            if (i+1)%self.copy_online_to_target_ep==0:\n",
    "                self.model_2 = self.model_1\n",
    "            \n",
    "            if (i+1)%self.eval_after==0:\n",
    "                r = self.run_env(env)\n",
    "                self.R.append(r)\n",
    "                \n",
    "                ecr = self.ecr_reward()\n",
    "                print(\"--epoch: {}/{} | ECR: {:.5f} | R: {:.2f} --\".format(i+1, epoch, ecr, r))\n",
    "        \n",
    "        self.model_2 = self.model_1\n",
    "        r = self.run_env(env)\n",
    "        self.R.append(r)\n",
    "\n",
    "        ecr = self.ecr_reward()\n",
    "        print(\"--epoch: {}/{} | ECR: {:.5f} | R: {:.2f} --\".format(i+1, epoch, ecr, r))\n",
    "    \n",
    "    \n",
    "    def _learn_minibatch(self):\n",
    "        priority_sum = self.batch['priority'].sum()\n",
    "        self.batch['weight'] = self.batch['priority']/priority_sum\n",
    "        \n",
    "        minibatch = self.batch.sample(self.minibatch_size, weights=self.batch['weight'])\n",
    "        \n",
    "        for i, row in minibatch.iterrows():\n",
    "            \n",
    "            state, action, reward, next_state, done = row['state'], row['action'], row['reward'], row['next_state'], row['done']\n",
    "            \n",
    "            target_q = reward\n",
    "            \n",
    "            state_array = row['state_array']\n",
    "            next_state_array = row['next_state_array']\n",
    "            \n",
    "            model_1 = True\n",
    "            # double_param==0 means Regular DQN with model_2 as Target\n",
    "            if np.random.rand() < self.double_param:\n",
    "                model_1 = False\n",
    "                \n",
    "            if not done:    \n",
    "                # double Q learning\n",
    "                if model_1:\n",
    "                    if self.mode==\"normal\":\n",
    "                        ns_act_values = self.model_1.predict(next_state_array)[0]\n",
    "                        a_prime = np.argmax(ns_act_values)\n",
    "                    elif self.mode==\"random\":\n",
    "                        a_prime = np.random.choice(range(self.action_size))\n",
    "                    else:\n",
    "                        a_prime = int(self.mode)\n",
    "                    a_prime = self._filter_bcq(row, a_prime)\n",
    "                    target_ns_act_values = self.model_2.predict(next_state_array)[0]\n",
    "                    target_ns_q = target_ns_act_values[a_prime]\n",
    "                else:\n",
    "                    if self.mode==\"normal\":\n",
    "                        ns_act_values = self.model_1.predict(next_state_array)[0]\n",
    "                        a_prime = np.argmax(ns_act_values)\n",
    "                    elif self.mode==\"random\":\n",
    "                        a_prime = np.random.choice(range(self.action_size))\n",
    "                    else:\n",
    "                        a_prime = int(self.mode)\n",
    "                    a_prime = self._filter_bcq(row, a_prime)\n",
    "                    target_ns_act_values = self.model_1.predict(next_state_array)[0]\n",
    "                    target_ns_q = target_ns_act_values[a_prime]                \n",
    "                \n",
    "                target_q = reward + self.gamma*target_ns_q\n",
    "                \n",
    "                self.batch.at[i, 'pred_action'] = a_prime\n",
    "                self.batch.at[i, 'pred_reward'] = target_q\n",
    "                \n",
    "            if model_1:\n",
    "                target_f = self.model_1.predict(state_array)\n",
    "                \n",
    "                # Prioritized Experience Reply with noise\n",
    "                self.batch.loc[i, 'priority'] = (abs(target_q - target_f[0][action]) + np.random.uniform(0, 0.001))**self.priority_aplha\n",
    "\n",
    "                target_f[0][action] = target_q\n",
    "                self.model_1.fit(state_array, target_f, epochs=1, verbose=0)\n",
    "            else:\n",
    "                target_f = self.model_2.predict(state_array)\n",
    "                \n",
    "                # Prioritized Experience Reply with noise\n",
    "                self.batch.loc[i, 'priority'] = (abs(target_q - target_f[0][action]) + np.random.uniform(0, 0.001))**self.priority_aplha\n",
    "\n",
    "                target_f[0][action] = target_q\n",
    "                self.model_2.fit(state_array, target_f, epochs=1, verbose=0) \n",
    "    \n",
    "    \n",
    "    \n",
    "    # TODO also, implement a predict loop first\n",
    "    def ecr_reward(self):\n",
    "        reward = 0.0\n",
    "        count = 0\n",
    "        for i, row in df_test.loc[df_test['transition_number']==0].iterrows():\n",
    "            state_array = row['state_array']\n",
    "            next_state_array = row['next_state_array']\n",
    "                \n",
    "            reward += self.act(state_array)[1]\n",
    "            count += 1\n",
    "            \n",
    "        ecr = reward/count\n",
    "        self.ecrs.append(ecr)\n",
    "        return ecr\n",
    "    \n",
    "    # TODO\n",
    "    def run_env(self, env):\n",
    "        if env is None:\n",
    "            return 0\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            state_array = np.array(state).reshape(1, self.state_size)\n",
    "            \n",
    "            action = self.act(state_array)[0]\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            state = next_state\n",
    "            if done:\n",
    "                state = env.reset()\n",
    "                prevs = []\n",
    "                for j in range(self.lookback):\n",
    "                    prevs.append((0,) * (self.state_size))\n",
    "                return reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
