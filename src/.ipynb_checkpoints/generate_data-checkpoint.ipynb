{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-06T18:26:36.510406Z",
     "start_time": "2020-11-06T18:26:36.319484Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0]\n",
      "Episode: 0 | Total Reward: -2.7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_id</th>\n",
       "      <th>transition_id</th>\n",
       "      <th>state</th>\n",
       "      <th>action</th>\n",
       "      <th>immediate_reward</th>\n",
       "      <th>delayed_reward</th>\n",
       "      <th>infer_reward</th>\n",
       "      <th>infer_reward_gp</th>\n",
       "      <th>done</th>\n",
       "      <th>next_state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>[1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>[1, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>[2, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>[3, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>[3, 2]</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>[3, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>[3, 1]</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>[3, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>[3, 2]</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>[3, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>[3, 1]</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>[3, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>[3, 2]</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>[3, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>[3, 3]</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>[3, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>[3, 4]</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>[3, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>[3, 3]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>[4, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>[4, 3]</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>[4, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>[4, 2]</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>[5, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>[5, 2]</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>[5, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>[5, 3]</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>[6, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>[6, 3]</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>[7, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>[7, 3]</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>[8, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>[8, 3]</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>[9, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>[9, 3]</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>[9, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>[9, 4]</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>[9, 5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>[9, 5]</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>[9, 6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>[9, 6]</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>[10, 6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>[10, 6]</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>[11, 6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>[11, 6]</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>[12, 6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>[12, 6]</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-2.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>[13, 6]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   episode_id transition_id    state action  immediate_reward delayed_reward  \\\n",
       "0           0             0   [0, 0]      0              -0.1              0   \n",
       "1           0             1   [0, 1]      1              -1.1              0   \n",
       "2           0             2   [1, 1]      0               0.9              0   \n",
       "3           0             3   [1, 2]      1              -0.1              0   \n",
       "4           0             4   [2, 2]      1              -0.1              0   \n",
       "5           0             5   [3, 2]      2              -0.1              0   \n",
       "6           0             6   [3, 1]      0              -0.1              0   \n",
       "7           0             7   [3, 2]      2              -0.1              0   \n",
       "8           0             8   [3, 1]      0              -0.1              0   \n",
       "9           0             9   [3, 2]      0              -0.1              0   \n",
       "10          0            10   [3, 3]      0              -0.1              0   \n",
       "11          0            11   [3, 4]      2              -0.1              0   \n",
       "12          0            12   [3, 3]      1               0.9              0   \n",
       "13          0            13   [4, 3]      2              -0.1              0   \n",
       "14          0            14   [4, 2]      2              -0.1              0   \n",
       "15          0            15   [5, 2]      0              -0.1              0   \n",
       "16          0            16   [5, 3]      1              -0.1              0   \n",
       "17          0            17   [6, 3]      1              -1.1              0   \n",
       "18          0            18   [7, 3]      1              -0.1              0   \n",
       "19          0            19   [8, 3]      1              -0.1              0   \n",
       "20          0            20   [9, 3]      0              -0.1              0   \n",
       "21          0            21   [9, 4]      1              -0.1              0   \n",
       "22          0            22   [9, 5]      0              -0.1              0   \n",
       "23          0            23   [9, 6]      0              -0.1              0   \n",
       "24          0            24  [10, 6]      1              -0.1              0   \n",
       "25          0            25  [11, 6]      2              -0.1              0   \n",
       "26          0            26  [12, 6]      0              -0.1           -2.7   \n",
       "\n",
       "    infer_reward  infer_reward_gp   done next_state  \n",
       "0            NaN              NaN  False     [0, 1]  \n",
       "1            NaN              NaN  False     [1, 1]  \n",
       "2            NaN              NaN  False     [1, 2]  \n",
       "3            NaN              NaN  False     [2, 2]  \n",
       "4            NaN              NaN  False     [3, 2]  \n",
       "5            NaN              NaN  False     [3, 1]  \n",
       "6            NaN              NaN  False     [3, 2]  \n",
       "7            NaN              NaN  False     [3, 1]  \n",
       "8            NaN              NaN  False     [3, 2]  \n",
       "9            NaN              NaN  False     [3, 3]  \n",
       "10           NaN              NaN  False     [3, 4]  \n",
       "11           NaN              NaN  False     [3, 3]  \n",
       "12           NaN              NaN  False     [4, 3]  \n",
       "13           NaN              NaN  False     [4, 2]  \n",
       "14           NaN              NaN  False     [5, 2]  \n",
       "15           NaN              NaN  False     [5, 3]  \n",
       "16           NaN              NaN  False     [6, 3]  \n",
       "17           NaN              NaN  False     [7, 3]  \n",
       "18           NaN              NaN  False     [8, 3]  \n",
       "19           NaN              NaN  False     [9, 3]  \n",
       "20           NaN              NaN  False     [9, 4]  \n",
       "21           NaN              NaN  False     [9, 5]  \n",
       "22           NaN              NaN  False     [9, 6]  \n",
       "23           NaN              NaN  False    [10, 6]  \n",
       "24           NaN              NaN  False    [11, 6]  \n",
       "25           NaN              NaN  False    [12, 6]  \n",
       "26           NaN              NaN   True    [13, 6]  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import gym_gridworld\n",
    "import pandas as pd\n",
    "\n",
    "random_state = 0\n",
    "np.random.seed(random_state)\n",
    "random.seed(random_state)\n",
    "\n",
    "env = gym.make('gridworld-v0', slide=True, deterministic=False)\n",
    "env.seed(random_state)\n",
    "action_size = 3\n",
    "\n",
    "episodes = 1\n",
    "df = pd.DataFrame(columns=['episode_id', 'transition_id', 'state', 'action', 'immediate_reward', \n",
    "                      'delayed_reward', 'infer_reward', 'infer_reward_gp', 'done', 'next_state'])\n",
    "\n",
    "for ep in range(episodes):\n",
    "    state = env.reset()\n",
    "    print(np.array(state))\n",
    "    done = False\n",
    "    delayed_reward = 0\n",
    "    transition_id = 0\n",
    "    while not done:\n",
    "#         env.render()\n",
    "        action = np.random.choice(range(action_size))\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        delayed_reward += reward\n",
    "\n",
    "        if done:\n",
    "            if ep%100==0:\n",
    "                print(\"Episode:\", ep, \"| Total Reward:\", round(delayed_reward,2))\n",
    "            df = df.append({'episode_id':ep, 'transition_id':transition_id, 'state':np.array(state), 'action':action, \n",
    "                   'immediate_reward': reward, 'delayed_reward':delayed_reward, 'done':done, 'next_state':np.array(next_state)}, \n",
    "                  ignore_index=True)\n",
    "            break\n",
    "        \n",
    "        df = df.append({'episode_id':ep, 'transition_id':transition_id, 'state':np.array(state), 'action':action, \n",
    "                   'immediate_reward': reward, 'delayed_reward':0, 'done':done, 'next_state':np.array(next_state)}, \n",
    "                  ignore_index=True)\n",
    "        transition_id += 1\n",
    "        state = next_state\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-06T02:34:50.865548Z",
     "start_time": "2020-11-06T02:34:50.562154Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total transitions: 35570  | Total episodes: 1000\n"
     ]
    }
   ],
   "source": [
    "df['delayed_reward'] = pd.to_numeric(df['delayed_reward'])\n",
    "df = df.sort_values(by=['episode_id', 'transition_id'])\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "print(\"Total transitions:\", len(df), \" | Total episodes:\", len(df['episode_id'].unique()))\n",
    "\n",
    "df.to_pickle('../data/gridworldchi_dm_slide_1k.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T20:34:03.920041Z",
     "start_time": "2020-11-04T20:34:03.773118Z"
    }
   },
   "source": [
    "# LSTM infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T22:09:50.768907Z",
     "start_time": "2020-11-05T22:09:46.389451Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fahmid/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/fahmid/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/fahmid/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/fahmid/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/fahmid/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/fahmid/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/fahmid/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/fahmid/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/fahmid/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/fahmid/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/fahmid/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/fahmid/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_id</th>\n",
       "      <th>transition_id</th>\n",
       "      <th>state</th>\n",
       "      <th>action</th>\n",
       "      <th>immediate_reward</th>\n",
       "      <th>delayed_reward</th>\n",
       "      <th>infer_reward</th>\n",
       "      <th>infer_reward_gp</th>\n",
       "      <th>done</th>\n",
       "      <th>next_state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>[0, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 2]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>[1, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>[1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>[1, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35935</th>\n",
       "      <td>999</td>\n",
       "      <td>34</td>\n",
       "      <td>[13, 1]</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>[13, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35936</th>\n",
       "      <td>999</td>\n",
       "      <td>35</td>\n",
       "      <td>[13, 2]</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>[13, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35937</th>\n",
       "      <td>999</td>\n",
       "      <td>36</td>\n",
       "      <td>[13, 3]</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>[13, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35938</th>\n",
       "      <td>999</td>\n",
       "      <td>37</td>\n",
       "      <td>[13, 4]</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>[13, 5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35939</th>\n",
       "      <td>999</td>\n",
       "      <td>38</td>\n",
       "      <td>[13, 5]</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-2.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>[13, 6]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35940 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      episode_id transition_id    state action  immediate_reward  \\\n",
       "0              0             0   [0, 0]      0              -0.1   \n",
       "1              0             1   [0, 1]      0              -0.1   \n",
       "2              0             2   [0, 2]      1               0.9   \n",
       "3              0             3   [1, 2]      2              -1.1   \n",
       "4              0             4   [1, 1]      2              -0.1   \n",
       "...          ...           ...      ...    ...               ...   \n",
       "35935        999            34  [13, 1]      0              -0.1   \n",
       "35936        999            35  [13, 2]      0              -0.1   \n",
       "35937        999            36  [13, 3]      0              -0.1   \n",
       "35938        999            37  [13, 4]      0              -0.1   \n",
       "35939        999            38  [13, 5]      0              -0.1   \n",
       "\n",
       "       delayed_reward  infer_reward  infer_reward_gp   done next_state  \n",
       "0                 0.0           NaN              NaN  False     [0, 1]  \n",
       "1                 0.0           NaN              NaN  False     [0, 2]  \n",
       "2                 0.0           NaN              NaN  False     [1, 2]  \n",
       "3                 0.0           NaN              NaN  False     [1, 1]  \n",
       "4                 0.0           NaN              NaN  False     [1, 2]  \n",
       "...               ...           ...              ...    ...        ...  \n",
       "35935             0.0           NaN              NaN  False    [13, 2]  \n",
       "35936             0.0           NaN              NaN  False    [13, 3]  \n",
       "35937             0.0           NaN              NaN  False    [13, 4]  \n",
       "35938             0.0           NaN              NaN  False    [13, 5]  \n",
       "35939            -2.9           NaN              NaN   True    [13, 6]  \n",
       "\n",
       "[35940 rows x 10 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, Model\n",
    "import keras.layers as layers\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras.layers.merge import _Merge, Multiply\n",
    "from copy import deepcopy\n",
    "import keras\n",
    "\n",
    "random_state=0\n",
    "np.random.seed(random_state)\n",
    "random.seed(random_state)\n",
    "\n",
    "dataset = '../data/gridworldchi_ndm_slide_1k.pkl'\n",
    "df = pd.read_pickle(dataset)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T22:09:50.812717Z",
     "start_time": "2020-11-05T22:09:50.771017Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History needed: 41 | Max step: 105.0 | Action size: 3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transition_id</th>\n",
       "      <th>state</th>\n",
       "      <th>action</th>\n",
       "      <th>immediate_reward</th>\n",
       "      <th>delayed_reward</th>\n",
       "      <th>infer_reward</th>\n",
       "      <th>infer_reward_gp</th>\n",
       "      <th>done</th>\n",
       "      <th>next_state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>35.940000</td>\n",
       "      <td>35.940000</td>\n",
       "      <td>35.940000</td>\n",
       "      <td>35.940000</td>\n",
       "      <td>35.940000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.940000</td>\n",
       "      <td>35.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>10.211595</td>\n",
       "      <td>10.211595</td>\n",
       "      <td>10.211595</td>\n",
       "      <td>10.211595</td>\n",
       "      <td>10.211595</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.211595</td>\n",
       "      <td>10.211595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>19.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>29.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>29.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>34.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>34.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>41.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>41.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>105.000000</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>105.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       transition_id        state       action  immediate_reward  \\\n",
       "count    1000.000000  1000.000000  1000.000000       1000.000000   \n",
       "mean       35.940000    35.940000    35.940000         35.940000   \n",
       "std        10.211595    10.211595    10.211595         10.211595   \n",
       "min        19.000000    19.000000    19.000000         19.000000   \n",
       "25%        29.000000    29.000000    29.000000         29.000000   \n",
       "50%        34.000000    34.000000    34.000000         34.000000   \n",
       "75%        41.000000    41.000000    41.000000         41.000000   \n",
       "max       105.000000   105.000000   105.000000        105.000000   \n",
       "\n",
       "       delayed_reward  infer_reward  infer_reward_gp         done   next_state  \n",
       "count     1000.000000        1000.0           1000.0  1000.000000  1000.000000  \n",
       "mean        35.940000           0.0              0.0    35.940000    35.940000  \n",
       "std         10.211595           0.0              0.0    10.211595    10.211595  \n",
       "min         19.000000           0.0              0.0    19.000000    19.000000  \n",
       "25%         29.000000           0.0              0.0    29.000000    29.000000  \n",
       "50%         34.000000           0.0              0.0    34.000000    34.000000  \n",
       "75%         41.000000           0.0              0.0    41.000000    41.000000  \n",
       "max        105.000000           0.0              0.0   105.000000   105.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summ = df.groupby(['episode_id']).count().describe()\n",
    "max_step = summ.loc['max','transition_id']\n",
    "history = int(summ.loc['75%','transition_id'])\n",
    "action_size = len(df['action'].unique())\n",
    "print(\"History needed:\", history, \"| Max step:\", max_step, \"| Action size:\", action_size)\n",
    "summ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T22:09:50.823400Z",
     "start_time": "2020-11-05T22:09:50.814399Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_features(row, action_size=action_size):\n",
    "    # dummy data for filler\n",
    "    if isinstance(row, int):\n",
    "        l = [0] * row\n",
    "        return [l]\n",
    "    \n",
    "    act_tuple = ()\n",
    "    for i in range(action_size):\n",
    "        if row['action']==i:\n",
    "            act_tuple += (1,)\n",
    "        else:\n",
    "            act_tuple += (0,)\n",
    "            \n",
    "    return [list(tuple([row['state'][0], row['state'][1]]) + tuple([row['next_state'][0], row['next_state'][1]]))]\n",
    "\n",
    "# LSTM MODEL\n",
    "def get_model(state_size, history, unit):\n",
    "    # one input layer\n",
    "    inp_s1 = layers.Input(shape=(None, state_size))\n",
    "    # an lstm layer that takes the input sequence, produces a sequence and returns states\n",
    "    layer_s1 = layers.LSTM(unit, return_sequences=True, return_state=True)\n",
    "    outputs_s1, state_h_s1, state_c_s1 = layer_s1(inp_s1)\n",
    "    # we use the states for next input sequence\n",
    "    states_prev = [state_h_s1, state_c_s1]\n",
    "    # and we use a dense as immediate reward\n",
    "    dense_s1 = layers.Dense(1, activation='linear')\n",
    "    out_s1 = dense_s1(outputs_s1)\n",
    "    \n",
    "    \n",
    "    outs = []\n",
    "    inps = []\n",
    "    outs.append(out_s1)\n",
    "    inps.append(inp_s1)\n",
    "    \n",
    "    for h in range(history-1):\n",
    "        # one input layer\n",
    "        inp_next = layers.Input(shape=(None, state_size))\n",
    "        # an lstm layer that takes the input sequence, produces a sequence and returns states\n",
    "        layer_next = layers.LSTM(unit, return_sequences=True, return_state=True)\n",
    "        outputs_next, state_h_next, state_c_next = layer_next(inp_next, initial_state=states_prev)\n",
    "        # we use the states for next input sequence\n",
    "        states_prev = [state_h_next, state_c_next]\n",
    "        # and we use a dense as immediate reward\n",
    "        dense_next = layers.Dense(1, activation='linear')\n",
    "        out_next = dense_next(outputs_next)\n",
    "\n",
    "        outs.append(out_next)\n",
    "        inps.append(inp_next)\n",
    "\n",
    "    # we have an upper layer for delayed reward\n",
    "    delayed = layers.Add()(outs)\n",
    "\n",
    "    model = Model(inps, delayed)\n",
    "    model.compile(loss='mse', optimizer=Adam(lr=0.01))\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T22:09:55.950498Z",
     "start_time": "2020-11-05T22:09:50.824932Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy feature: [[3, 3, 4, 3]]\n"
     ]
    }
   ],
   "source": [
    "row = df.loc[12]\n",
    "dummy = get_features(row)\n",
    "print(\"Dummy feature:\", dummy)\n",
    "\n",
    "\n",
    "# Create INPUT OUTPUT for LSTM\n",
    "feature_length = len(get_features(df.loc[0])[0])\n",
    "\n",
    "X = []\n",
    "for i in range(history):\n",
    "    X.append([])\n",
    "    \n",
    "y = []\n",
    "for ep in sorted(df['episode_id'].unique()):\n",
    "    step_count = 0\n",
    "    for i, row in df.loc[df['episode_id']==ep].sort_values('transition_id', ascending=False).iterrows():\n",
    "        if step_count==0:\n",
    "            y.append([[row['delayed_reward']]])\n",
    "\n",
    "        feature = get_features(row)\n",
    "        X[step_count].append(feature)\n",
    "        step_count+=1\n",
    "        if step_count==history:\n",
    "            break\n",
    "    while step_count<history:\n",
    "        dummy = get_features(feature_length)\n",
    "        X[step_count].append(dummy)\n",
    "        step_count+=1\n",
    "    \n",
    "y = np.array(y)\n",
    "X.reverse()\n",
    "for i in range(history):\n",
    "    X[i] = np.array(X[i])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T22:09:55.961015Z",
     "start_time": "2020-11-05T22:09:55.952238Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_and_optimize(X, y, df):\n",
    "    best_error = 99999\n",
    "    best_unit = 8\n",
    "    best_df = df.copy()\n",
    "    for unit in [8, 16, 24, 32]:\n",
    "        # Build a model\n",
    "        print('--running ' + str(unit) + '--')\n",
    "        model = get_model(state_size=feature_length, history=history, unit=unit)\n",
    "        model.fit(X,y, epochs=200)\n",
    "\n",
    "        # This is the infer reward\n",
    "        f = keras.backend.function(model.input, model.layers[-1].input) \n",
    "        infer_reward = f([X,1])\n",
    "        \n",
    "        print(\"--calculating error--\")\n",
    "        df['infer_reward'] = 0\n",
    "        for ep in sorted(df['episode_id'].unique()):\n",
    "            ep_total = 0\n",
    "            if ep%100==0:\n",
    "                print(\"Copying immediate reward for Ep:\", ep)\n",
    "\n",
    "            curr_trans = max(df.loc[df['episode_id']==ep, 'transition_id'])\n",
    "            # this loops from last transitions to back\n",
    "            for h in reversed(range(history)):\n",
    "                immediate_reward = infer_reward[h][ep][0][0]\n",
    "                ep_total += immediate_reward\n",
    "                if curr_trans>=0:\n",
    "                    df.loc[(df['episode_id']==ep) & (df['transition_id']==curr_trans), 'infer_reward'] = immediate_reward\n",
    "                    curr_trans-=1\n",
    "                    \n",
    "        a = df.groupby(['episode_id']).sum()\n",
    "        mae_delayed = sum(abs(a['delayed_reward'] - a['infer_reward']))/len(a)\n",
    "        mae_imm = sum(abs(df['immediate_reward'] - df['infer_reward']))/len(df)\n",
    "        print(\"MAE Error Delayed:\", mae_delayed)\n",
    "        print(\"MAE Error Immediate:\", mae_imm)\n",
    "        if mae_delayed<best_error:\n",
    "            best_unit = unit\n",
    "            best_error = mae_delayed\n",
    "            best_df = df.copy()\n",
    "    return best_unit, best_error, best_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T22:34:39.392257Z",
     "start_time": "2020-11-05T22:09:55.964149Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--running 8--\n",
      "WARNING:tensorflow:From /Users/fahmid/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/200\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 5.4577\n",
      "Epoch 2/200\n",
      "1000/1000 [==============================] - 1s 827us/step - loss: 1.4066\n",
      "Epoch 3/200\n",
      "1000/1000 [==============================] - 1s 844us/step - loss: 1.2907\n",
      "Epoch 4/200\n",
      "1000/1000 [==============================] - 1s 824us/step - loss: 1.2617\n",
      "Epoch 5/200\n",
      "1000/1000 [==============================] - 1s 836us/step - loss: 1.1712\n",
      "Epoch 6/200\n",
      "1000/1000 [==============================] - 1s 838us/step - loss: 1.2726\n",
      "Epoch 7/200\n",
      "1000/1000 [==============================] - 1s 848us/step - loss: 1.1234\n",
      "Epoch 8/200\n",
      "1000/1000 [==============================] - 1s 903us/step - loss: 1.1079\n",
      "Epoch 9/200\n",
      "1000/1000 [==============================] - 1s 927us/step - loss: 1.0711\n",
      "Epoch 10/200\n",
      "1000/1000 [==============================] - 1s 919us/step - loss: 0.9921\n",
      "Epoch 11/200\n",
      "1000/1000 [==============================] - 1s 925us/step - loss: 0.9247\n",
      "Epoch 12/200\n",
      "1000/1000 [==============================] - 1s 919us/step - loss: 0.9039\n",
      "Epoch 13/200\n",
      "1000/1000 [==============================] - 1s 913us/step - loss: 0.9745\n",
      "Epoch 14/200\n",
      "1000/1000 [==============================] - 1s 906us/step - loss: 0.9751\n",
      "Epoch 15/200\n",
      "1000/1000 [==============================] - 1s 892us/step - loss: 0.8388\n",
      "Epoch 16/200\n",
      "1000/1000 [==============================] - 1s 896us/step - loss: 0.8574\n",
      "Epoch 17/200\n",
      "1000/1000 [==============================] - 1s 884us/step - loss: 0.9945\n",
      "Epoch 18/200\n",
      "1000/1000 [==============================] - 1s 880us/step - loss: 0.8865\n",
      "Epoch 19/200\n",
      "1000/1000 [==============================] - 1s 873us/step - loss: 0.7806\n",
      "Epoch 20/200\n",
      "1000/1000 [==============================] - 1s 885us/step - loss: 0.7171\n",
      "Epoch 21/200\n",
      "1000/1000 [==============================] - 1s 884us/step - loss: 0.7163\n",
      "Epoch 22/200\n",
      "1000/1000 [==============================] - 1s 882us/step - loss: 0.7411\n",
      "Epoch 23/200\n",
      "1000/1000 [==============================] - 1s 883us/step - loss: 0.6745\n",
      "Epoch 24/200\n",
      "1000/1000 [==============================] - 1s 892us/step - loss: 0.6826\n",
      "Epoch 25/200\n",
      "1000/1000 [==============================] - 1s 863us/step - loss: 0.7005\n",
      "Epoch 26/200\n",
      "1000/1000 [==============================] - 1s 857us/step - loss: 0.7284\n",
      "Epoch 27/200\n",
      "1000/1000 [==============================] - 1s 878us/step - loss: 0.7943\n",
      "Epoch 28/200\n",
      "1000/1000 [==============================] - 1s 881us/step - loss: 0.6729\n",
      "Epoch 29/200\n",
      "1000/1000 [==============================] - 1s 884us/step - loss: 0.6862\n",
      "Epoch 30/200\n",
      "1000/1000 [==============================] - 1s 881us/step - loss: 0.5578\n",
      "Epoch 31/200\n",
      "1000/1000 [==============================] - 1s 893us/step - loss: 0.6198\n",
      "Epoch 32/200\n",
      "1000/1000 [==============================] - 1s 883us/step - loss: 0.6032\n",
      "Epoch 33/200\n",
      "1000/1000 [==============================] - 1s 905us/step - loss: 0.5619\n",
      "Epoch 34/200\n",
      "1000/1000 [==============================] - 1s 898us/step - loss: 0.5396\n",
      "Epoch 35/200\n",
      "1000/1000 [==============================] - 1s 911us/step - loss: 0.5398\n",
      "Epoch 36/200\n",
      "1000/1000 [==============================] - 1s 896us/step - loss: 0.6120\n",
      "Epoch 37/200\n",
      "1000/1000 [==============================] - 1s 886us/step - loss: 0.5866\n",
      "Epoch 38/200\n",
      "1000/1000 [==============================] - 1s 881us/step - loss: 0.5243\n",
      "Epoch 39/200\n",
      "1000/1000 [==============================] - 1s 879us/step - loss: 0.4954\n",
      "Epoch 40/200\n",
      "1000/1000 [==============================] - 1s 879us/step - loss: 0.4502\n",
      "Epoch 41/200\n",
      "1000/1000 [==============================] - 1s 856us/step - loss: 0.5578\n",
      "Epoch 42/200\n",
      "1000/1000 [==============================] - 1s 864us/step - loss: 0.4850\n",
      "Epoch 43/200\n",
      "1000/1000 [==============================] - 1s 884us/step - loss: 0.4538\n",
      "Epoch 44/200\n",
      "1000/1000 [==============================] - 1s 882us/step - loss: 0.4518\n",
      "Epoch 45/200\n",
      "1000/1000 [==============================] - 1s 884us/step - loss: 0.4754\n",
      "Epoch 46/200\n",
      "1000/1000 [==============================] - 1s 908us/step - loss: 0.3865\n",
      "Epoch 47/200\n",
      "1000/1000 [==============================] - 1s 919us/step - loss: 0.4015\n",
      "Epoch 48/200\n",
      "1000/1000 [==============================] - 1s 901us/step - loss: 0.3995\n",
      "Epoch 49/200\n",
      "1000/1000 [==============================] - 1s 881us/step - loss: 0.4471\n",
      "Epoch 50/200\n",
      "1000/1000 [==============================] - 1s 886us/step - loss: 0.4238\n",
      "Epoch 51/200\n",
      "1000/1000 [==============================] - 1s 887us/step - loss: 0.4023\n",
      "Epoch 52/200\n",
      "1000/1000 [==============================] - 1s 882us/step - loss: 0.3514\n",
      "Epoch 53/200\n",
      "1000/1000 [==============================] - 1s 880us/step - loss: 0.3657\n",
      "Epoch 54/200\n",
      "1000/1000 [==============================] - 1s 885us/step - loss: 0.3937\n",
      "Epoch 55/200\n",
      "1000/1000 [==============================] - 1s 881us/step - loss: 0.3704\n",
      "Epoch 56/200\n",
      "1000/1000 [==============================] - 1s 884us/step - loss: 0.4607\n",
      "Epoch 57/200\n",
      "1000/1000 [==============================] - 1s 882us/step - loss: 0.3847\n",
      "Epoch 58/200\n",
      "1000/1000 [==============================] - 1s 889us/step - loss: 0.4177\n",
      "Epoch 59/200\n",
      "1000/1000 [==============================] - 1s 890us/step - loss: 0.3686\n",
      "Epoch 60/200\n",
      "1000/1000 [==============================] - 1s 884us/step - loss: 0.3918\n",
      "Epoch 61/200\n",
      "1000/1000 [==============================] - 1s 882us/step - loss: 0.3809\n",
      "Epoch 62/200\n",
      "1000/1000 [==============================] - 1s 885us/step - loss: 0.3408\n",
      "Epoch 63/200\n",
      "1000/1000 [==============================] - 1s 890us/step - loss: 0.3714\n",
      "Epoch 64/200\n",
      "1000/1000 [==============================] - 1s 890us/step - loss: 0.3374\n",
      "Epoch 65/200\n",
      "1000/1000 [==============================] - 1s 888us/step - loss: 0.2849\n",
      "Epoch 66/200\n",
      "1000/1000 [==============================] - 1s 913us/step - loss: 0.2894\n",
      "Epoch 67/200\n",
      "1000/1000 [==============================] - 1s 914us/step - loss: 0.3159\n",
      "Epoch 68/200\n",
      "1000/1000 [==============================] - 1s 910us/step - loss: 0.2940\n",
      "Epoch 69/200\n",
      "1000/1000 [==============================] - 1s 887us/step - loss: 0.3477\n",
      "Epoch 70/200\n",
      "1000/1000 [==============================] - 1s 882us/step - loss: 0.2991\n",
      "Epoch 71/200\n",
      "1000/1000 [==============================] - 1s 885us/step - loss: 0.2467\n",
      "Epoch 72/200\n",
      "1000/1000 [==============================] - 1s 897us/step - loss: 0.3073\n",
      "Epoch 73/200\n",
      "1000/1000 [==============================] - 1s 883us/step - loss: 0.3247\n",
      "Epoch 74/200\n",
      "1000/1000 [==============================] - 1s 899us/step - loss: 0.2826\n",
      "Epoch 75/200\n",
      "1000/1000 [==============================] - 1s 888us/step - loss: 0.2608\n",
      "Epoch 76/200\n",
      "1000/1000 [==============================] - 1s 895us/step - loss: 0.2496\n",
      "Epoch 77/200\n",
      "1000/1000 [==============================] - 1s 916us/step - loss: 0.2340\n",
      "Epoch 78/200\n",
      "1000/1000 [==============================] - 1s 911us/step - loss: 0.2678\n",
      "Epoch 79/200\n",
      "1000/1000 [==============================] - 1s 909us/step - loss: 0.3238\n",
      "Epoch 80/200\n",
      "1000/1000 [==============================] - 1s 904us/step - loss: 0.3066\n",
      "Epoch 81/200\n",
      "1000/1000 [==============================] - 1s 888us/step - loss: 0.2520\n",
      "Epoch 82/200\n",
      "1000/1000 [==============================] - 1s 902us/step - loss: 0.2031\n",
      "Epoch 83/200\n",
      "1000/1000 [==============================] - 1s 892us/step - loss: 0.2255\n",
      "Epoch 84/200\n",
      "1000/1000 [==============================] - 1s 898us/step - loss: 0.2964\n",
      "Epoch 85/200\n",
      "1000/1000 [==============================] - 1s 904us/step - loss: 0.3084\n",
      "Epoch 86/200\n",
      "1000/1000 [==============================] - 1s 909us/step - loss: 0.2780\n",
      "Epoch 87/200\n",
      "1000/1000 [==============================] - 1s 914us/step - loss: 0.3349\n",
      "Epoch 88/200\n",
      "1000/1000 [==============================] - 1s 914us/step - loss: 0.3181\n",
      "Epoch 89/200\n",
      "1000/1000 [==============================] - 1s 911us/step - loss: 0.2347\n",
      "Epoch 90/200\n",
      "1000/1000 [==============================] - 1s 910us/step - loss: 0.1887\n",
      "Epoch 91/200\n",
      "1000/1000 [==============================] - 1s 941us/step - loss: 0.1901\n",
      "Epoch 92/200\n",
      "1000/1000 [==============================] - 1s 948us/step - loss: 0.2206\n",
      "Epoch 93/200\n",
      "1000/1000 [==============================] - 1s 939us/step - loss: 0.2205\n",
      "Epoch 94/200\n",
      "1000/1000 [==============================] - 1s 965us/step - loss: 0.1798\n",
      "Epoch 95/200\n",
      "1000/1000 [==============================] - 1s 964us/step - loss: 0.2273\n",
      "Epoch 96/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1881\n",
      "Epoch 97/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2024\n",
      "Epoch 98/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1615\n",
      "Epoch 99/200\n",
      "1000/1000 [==============================] - 1s 1000us/step - loss: 0.1776\n",
      "Epoch 100/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1923\n",
      "Epoch 101/200\n",
      "1000/1000 [==============================] - 1s 999us/step - loss: 0.2450\n",
      "Epoch 102/200\n",
      "1000/1000 [==============================] - 1s 971us/step - loss: 0.2059\n",
      "Epoch 103/200\n",
      "1000/1000 [==============================] - 1s 967us/step - loss: 0.2003\n",
      "Epoch 104/200\n",
      "1000/1000 [==============================] - 1s 969us/step - loss: 0.2183\n",
      "Epoch 105/200\n",
      "1000/1000 [==============================] - 1s 936us/step - loss: 0.2109\n",
      "Epoch 106/200\n",
      "1000/1000 [==============================] - 1s 949us/step - loss: 0.1915\n",
      "Epoch 107/200\n",
      "1000/1000 [==============================] - 1s 943us/step - loss: 0.1677\n",
      "Epoch 108/200\n",
      "1000/1000 [==============================] - 1s 940us/step - loss: 0.1674\n",
      "Epoch 109/200\n",
      "1000/1000 [==============================] - 1s 941us/step - loss: 0.1593\n",
      "Epoch 110/200\n",
      "1000/1000 [==============================] - 1s 942us/step - loss: 0.1440\n",
      "Epoch 111/200\n",
      "1000/1000 [==============================] - 1s 958us/step - loss: 0.1402\n",
      "Epoch 112/200\n",
      "1000/1000 [==============================] - 1s 936us/step - loss: 0.1411\n",
      "Epoch 113/200\n",
      "1000/1000 [==============================] - 1s 944us/step - loss: 0.1422\n",
      "Epoch 114/200\n",
      "1000/1000 [==============================] - 1s 937us/step - loss: 0.1380\n",
      "Epoch 115/200\n",
      "1000/1000 [==============================] - 1s 936us/step - loss: 0.1511\n",
      "Epoch 116/200\n",
      "1000/1000 [==============================] - 1s 941us/step - loss: 0.1877\n",
      "Epoch 117/200\n",
      "1000/1000 [==============================] - 1s 941us/step - loss: 0.1585\n",
      "Epoch 118/200\n",
      "1000/1000 [==============================] - 1s 932us/step - loss: 0.1370\n",
      "Epoch 119/200\n",
      "1000/1000 [==============================] - 1s 942us/step - loss: 0.1642\n",
      "Epoch 120/200\n",
      "1000/1000 [==============================] - 1s 968us/step - loss: 0.1389\n",
      "Epoch 121/200\n",
      "1000/1000 [==============================] - 1s 969us/step - loss: 0.1233\n",
      "Epoch 122/200\n",
      "1000/1000 [==============================] - 1s 969us/step - loss: 0.1238\n",
      "Epoch 123/200\n",
      "1000/1000 [==============================] - 1s 983us/step - loss: 0.1300\n",
      "Epoch 124/200\n",
      "1000/1000 [==============================] - 1s 966us/step - loss: 0.1436\n",
      "Epoch 125/200\n",
      "1000/1000 [==============================] - 1s 972us/step - loss: 0.1385\n",
      "Epoch 126/200\n",
      "1000/1000 [==============================] - 1s 963us/step - loss: 0.1661\n",
      "Epoch 127/200\n",
      "1000/1000 [==============================] - 1s 970us/step - loss: 0.1582\n",
      "Epoch 128/200\n",
      "1000/1000 [==============================] - 1s 968us/step - loss: 0.1521\n",
      "Epoch 129/200\n",
      "1000/1000 [==============================] - 1s 965us/step - loss: 0.1586\n",
      "Epoch 130/200\n",
      "1000/1000 [==============================] - 1s 971us/step - loss: 0.1548\n",
      "Epoch 131/200\n",
      "1000/1000 [==============================] - 1s 971us/step - loss: 0.1287\n",
      "Epoch 132/200\n",
      "1000/1000 [==============================] - 1s 965us/step - loss: 0.1068\n",
      "Epoch 133/200\n",
      "1000/1000 [==============================] - 1s 939us/step - loss: 0.1388\n",
      "Epoch 134/200\n",
      "1000/1000 [==============================] - 1s 937us/step - loss: 0.1556\n",
      "Epoch 135/200\n",
      "1000/1000 [==============================] - 1s 935us/step - loss: 0.1498\n",
      "Epoch 136/200\n",
      "1000/1000 [==============================] - 1s 940us/step - loss: 0.1401\n",
      "Epoch 137/200\n",
      "1000/1000 [==============================] - 1s 933us/step - loss: 0.1323\n",
      "Epoch 138/200\n",
      "1000/1000 [==============================] - 1s 942us/step - loss: 0.1387\n",
      "Epoch 139/200\n",
      "1000/1000 [==============================] - 1s 932us/step - loss: 0.1075\n",
      "Epoch 140/200\n",
      "1000/1000 [==============================] - 1s 932us/step - loss: 0.1045\n",
      "Epoch 141/200\n",
      "1000/1000 [==============================] - 1s 936us/step - loss: 0.1309\n",
      "Epoch 142/200\n",
      "1000/1000 [==============================] - 1s 936us/step - loss: 0.1551\n",
      "Epoch 143/200\n",
      "1000/1000 [==============================] - 1s 932us/step - loss: 0.1704\n",
      "Epoch 144/200\n",
      "1000/1000 [==============================] - 1s 952us/step - loss: 0.1803\n",
      "Epoch 145/200\n",
      "1000/1000 [==============================] - 1s 931us/step - loss: 0.1252\n",
      "Epoch 146/200\n",
      "1000/1000 [==============================] - 1s 941us/step - loss: 0.1367\n",
      "Epoch 147/200\n",
      "1000/1000 [==============================] - 1s 933us/step - loss: 0.1311\n",
      "Epoch 148/200\n",
      "1000/1000 [==============================] - 1s 938us/step - loss: 0.1284\n",
      "Epoch 149/200\n",
      "1000/1000 [==============================] - 1s 933us/step - loss: 0.0987\n",
      "Epoch 150/200\n",
      "1000/1000 [==============================] - 1s 953us/step - loss: 0.1120\n",
      "Epoch 151/200\n",
      "1000/1000 [==============================] - 1s 969us/step - loss: 0.1050\n",
      "Epoch 152/200\n",
      "1000/1000 [==============================] - 1s 966us/step - loss: 0.0928\n",
      "Epoch 153/200\n",
      "1000/1000 [==============================] - 1s 967us/step - loss: 0.0941\n",
      "Epoch 154/200\n",
      "1000/1000 [==============================] - 1s 969us/step - loss: 0.1178\n",
      "Epoch 155/200\n",
      "1000/1000 [==============================] - 1s 969us/step - loss: 0.1230\n",
      "Epoch 156/200\n",
      "1000/1000 [==============================] - 1s 964us/step - loss: 0.1339\n",
      "Epoch 157/200\n",
      "1000/1000 [==============================] - 1s 966us/step - loss: 0.1537\n",
      "Epoch 158/200\n",
      "1000/1000 [==============================] - 1s 973us/step - loss: 0.1768\n",
      "Epoch 159/200\n",
      "1000/1000 [==============================] - 1s 964us/step - loss: 0.1489\n",
      "Epoch 160/200\n",
      "1000/1000 [==============================] - 1s 965us/step - loss: 0.1321\n",
      "Epoch 161/200\n",
      "1000/1000 [==============================] - 1s 964us/step - loss: 0.1383\n",
      "Epoch 162/200\n",
      "1000/1000 [==============================] - 1s 964us/step - loss: 0.1562\n",
      "Epoch 163/200\n",
      "1000/1000 [==============================] - 1s 965us/step - loss: 0.1445\n",
      "Epoch 164/200\n",
      "1000/1000 [==============================] - 1s 973us/step - loss: 0.1463\n",
      "Epoch 165/200\n",
      "1000/1000 [==============================] - 1s 964us/step - loss: 0.1222\n",
      "Epoch 166/200\n",
      "1000/1000 [==============================] - 1s 969us/step - loss: 0.1344\n",
      "Epoch 167/200\n",
      "1000/1000 [==============================] - 1s 936us/step - loss: 0.1626\n",
      "Epoch 168/200\n",
      "1000/1000 [==============================] - 1s 946us/step - loss: 0.1765\n",
      "Epoch 169/200\n",
      "1000/1000 [==============================] - 1s 941us/step - loss: 0.1266\n",
      "Epoch 170/200\n",
      "1000/1000 [==============================] - 1s 934us/step - loss: 0.1235\n",
      "Epoch 171/200\n",
      "1000/1000 [==============================] - 1s 934us/step - loss: 0.1379\n",
      "Epoch 172/200\n",
      "1000/1000 [==============================] - 1s 931us/step - loss: 0.1073\n",
      "Epoch 173/200\n",
      "1000/1000 [==============================] - 1s 934us/step - loss: 0.0841\n",
      "Epoch 174/200\n",
      "1000/1000 [==============================] - 1s 943us/step - loss: 0.1100\n",
      "Epoch 175/200\n",
      "1000/1000 [==============================] - 1s 938us/step - loss: 0.1131\n",
      "Epoch 176/200\n",
      "1000/1000 [==============================] - 1s 960us/step - loss: 0.1029\n",
      "Epoch 177/200\n",
      "1000/1000 [==============================] - 1s 938us/step - loss: 0.1080\n",
      "Epoch 178/200\n",
      "1000/1000 [==============================] - 1s 932us/step - loss: 0.0919\n",
      "Epoch 179/200\n",
      "1000/1000 [==============================] - 1s 940us/step - loss: 0.0784\n",
      "Epoch 180/200\n",
      "1000/1000 [==============================] - 1s 942us/step - loss: 0.0829\n",
      "Epoch 181/200\n",
      "1000/1000 [==============================] - 1s 934us/step - loss: 0.0758\n",
      "Epoch 182/200\n",
      "1000/1000 [==============================] - 1s 965us/step - loss: 0.0838\n",
      "Epoch 183/200\n",
      "1000/1000 [==============================] - 1s 960us/step - loss: 0.0971\n",
      "Epoch 184/200\n",
      "1000/1000 [==============================] - 1s 972us/step - loss: 0.1176\n",
      "Epoch 185/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 1s 975us/step - loss: 0.1574\n",
      "Epoch 186/200\n",
      "1000/1000 [==============================] - 1s 964us/step - loss: 0.1777\n",
      "Epoch 187/200\n",
      "1000/1000 [==============================] - 1s 962us/step - loss: 0.1285\n",
      "Epoch 188/200\n",
      "1000/1000 [==============================] - 1s 970us/step - loss: 0.1535\n",
      "Epoch 189/200\n",
      "1000/1000 [==============================] - 1s 978us/step - loss: 0.1323\n",
      "Epoch 190/200\n",
      "1000/1000 [==============================] - 1s 972us/step - loss: 0.1461\n",
      "Epoch 191/200\n",
      "1000/1000 [==============================] - 1s 975us/step - loss: 0.1799\n",
      "Epoch 192/200\n",
      "1000/1000 [==============================] - 1s 961us/step - loss: 0.1636\n",
      "Epoch 193/200\n",
      "1000/1000 [==============================] - 1s 972us/step - loss: 0.1456\n",
      "Epoch 194/200\n",
      "1000/1000 [==============================] - 1s 981us/step - loss: 0.0973\n",
      "Epoch 195/200\n",
      "1000/1000 [==============================] - 1s 966us/step - loss: 0.1039\n",
      "Epoch 196/200\n",
      "1000/1000 [==============================] - 1s 967us/step - loss: 0.1028\n",
      "Epoch 197/200\n",
      "1000/1000 [==============================] - 1s 968us/step - loss: 0.0906\n",
      "Epoch 198/200\n",
      "1000/1000 [==============================] - 1s 946us/step - loss: 0.0881\n",
      "Epoch 199/200\n",
      "1000/1000 [==============================] - 1s 940us/step - loss: 0.1087\n",
      "Epoch 200/200\n",
      "1000/1000 [==============================] - 1s 943us/step - loss: 0.0939\n",
      "--calculating error--\n",
      "Copying immediate reward for Ep: 0\n",
      "Copying immediate reward for Ep: 100\n",
      "Copying immediate reward for Ep: 200\n",
      "Copying immediate reward for Ep: 300\n",
      "Copying immediate reward for Ep: 400\n",
      "Copying immediate reward for Ep: 500\n",
      "Copying immediate reward for Ep: 600\n",
      "Copying immediate reward for Ep: 700\n",
      "Copying immediate reward for Ep: 800\n",
      "Copying immediate reward for Ep: 900\n",
      "MAE Error Delayed: 0.22344178561456454\n",
      "MAE Error Immediate: 0.3888679167158371\n",
      "--running 16--\n",
      "Epoch 1/200\n",
      "1000/1000 [==============================] - 14s 14ms/step - loss: 6.5856\n",
      "Epoch 2/200\n",
      "1000/1000 [==============================] - 1s 852us/step - loss: 1.4550\n",
      "Epoch 3/200\n",
      "1000/1000 [==============================] - 1s 873us/step - loss: 1.4300\n",
      "Epoch 4/200\n",
      "1000/1000 [==============================] - 1s 842us/step - loss: 1.3331\n",
      "Epoch 5/200\n",
      "1000/1000 [==============================] - 1s 849us/step - loss: 1.3415\n",
      "Epoch 6/200\n",
      "1000/1000 [==============================] - 1s 856us/step - loss: 1.1144\n",
      "Epoch 7/200\n",
      "1000/1000 [==============================] - 1s 853us/step - loss: 1.1292\n",
      "Epoch 8/200\n",
      "1000/1000 [==============================] - 1s 897us/step - loss: 1.0649\n",
      "Epoch 9/200\n",
      "1000/1000 [==============================] - 1s 915us/step - loss: 1.0603\n",
      "Epoch 10/200\n",
      "1000/1000 [==============================] - 1s 934us/step - loss: 1.0099\n",
      "Epoch 11/200\n",
      "1000/1000 [==============================] - 1s 942us/step - loss: 1.0393\n",
      "Epoch 12/200\n",
      "1000/1000 [==============================] - 1s 992us/step - loss: 1.0281\n",
      "Epoch 13/200\n",
      "1000/1000 [==============================] - 1s 934us/step - loss: 1.0089\n",
      "Epoch 14/200\n",
      "1000/1000 [==============================] - 1s 919us/step - loss: 1.0094\n",
      "Epoch 15/200\n",
      "1000/1000 [==============================] - 1s 910us/step - loss: 0.9207\n",
      "Epoch 16/200\n",
      "1000/1000 [==============================] - 1s 893us/step - loss: 0.9544\n",
      "Epoch 17/200\n",
      "1000/1000 [==============================] - 1s 897us/step - loss: 0.9353\n",
      "Epoch 18/200\n",
      "1000/1000 [==============================] - 1s 906us/step - loss: 0.8914\n",
      "Epoch 19/200\n",
      "1000/1000 [==============================] - 1s 912us/step - loss: 0.7601\n",
      "Epoch 20/200\n",
      "1000/1000 [==============================] - 1s 928us/step - loss: 0.7559\n",
      "Epoch 21/200\n",
      "1000/1000 [==============================] - 1s 914us/step - loss: 0.7388\n",
      "Epoch 22/200\n",
      "1000/1000 [==============================] - 1s 920us/step - loss: 0.8163\n",
      "Epoch 23/200\n",
      "1000/1000 [==============================] - 1s 913us/step - loss: 0.7587\n",
      "Epoch 24/200\n",
      "1000/1000 [==============================] - 1s 917us/step - loss: 0.7311\n",
      "Epoch 25/200\n",
      "1000/1000 [==============================] - 1s 925us/step - loss: 0.7173\n",
      "Epoch 26/200\n",
      "1000/1000 [==============================] - 1s 934us/step - loss: 0.7235\n",
      "Epoch 27/200\n",
      "1000/1000 [==============================] - 1s 916us/step - loss: 0.6746\n",
      "Epoch 28/200\n",
      "1000/1000 [==============================] - 1s 925us/step - loss: 0.6576\n",
      "Epoch 29/200\n",
      "1000/1000 [==============================] - 1s 928us/step - loss: 0.6908\n",
      "Epoch 30/200\n",
      "1000/1000 [==============================] - 1s 915us/step - loss: 0.6428\n",
      "Epoch 31/200\n",
      "1000/1000 [==============================] - 1s 956us/step - loss: 0.6543\n",
      "Epoch 32/200\n",
      "1000/1000 [==============================] - 1s 921us/step - loss: 0.5502\n",
      "Epoch 33/200\n",
      "1000/1000 [==============================] - 1s 915us/step - loss: 0.5412\n",
      "Epoch 34/200\n",
      "1000/1000 [==============================] - 1s 915us/step - loss: 0.5271\n",
      "Epoch 35/200\n",
      "1000/1000 [==============================] - 1s 916us/step - loss: 0.6248\n",
      "Epoch 36/200\n",
      "1000/1000 [==============================] - 1s 921us/step - loss: 0.5118\n",
      "Epoch 37/200\n",
      "1000/1000 [==============================] - 1s 926us/step - loss: 0.5107\n",
      "Epoch 38/200\n",
      "1000/1000 [==============================] - 1s 927us/step - loss: 0.4609\n",
      "Epoch 39/200\n",
      "1000/1000 [==============================] - 1s 920us/step - loss: 0.5097\n",
      "Epoch 40/200\n",
      "1000/1000 [==============================] - 1s 918us/step - loss: 0.5295\n",
      "Epoch 41/200\n",
      "1000/1000 [==============================] - 1s 918us/step - loss: 0.4339\n",
      "Epoch 42/200\n",
      "1000/1000 [==============================] - 1s 916us/step - loss: 0.4215\n",
      "Epoch 43/200\n",
      "1000/1000 [==============================] - 1s 918us/step - loss: 0.3834\n",
      "Epoch 44/200\n",
      "1000/1000 [==============================] - 1s 914us/step - loss: 0.4314\n",
      "Epoch 45/200\n",
      "1000/1000 [==============================] - 1s 916us/step - loss: 0.4162\n",
      "Epoch 46/200\n",
      "1000/1000 [==============================] - 1s 916us/step - loss: 0.5960\n",
      "Epoch 47/200\n",
      "1000/1000 [==============================] - 1s 924us/step - loss: 0.4182\n",
      "Epoch 48/200\n",
      "1000/1000 [==============================] - 1s 944us/step - loss: 0.3934\n",
      "Epoch 49/200\n",
      "1000/1000 [==============================] - 1s 944us/step - loss: 0.3458\n",
      "Epoch 50/200\n",
      "1000/1000 [==============================] - 1s 923us/step - loss: 0.3660\n",
      "Epoch 51/200\n",
      "1000/1000 [==============================] - 1s 914us/step - loss: 0.4272\n",
      "Epoch 52/200\n",
      "1000/1000 [==============================] - 1s 919us/step - loss: 0.4539\n",
      "Epoch 53/200\n",
      "1000/1000 [==============================] - 1s 925us/step - loss: 0.4969\n",
      "Epoch 54/200\n",
      "1000/1000 [==============================] - 1s 920us/step - loss: 0.4370\n",
      "Epoch 55/200\n",
      "1000/1000 [==============================] - 1s 915us/step - loss: 0.3134\n",
      "Epoch 56/200\n",
      "1000/1000 [==============================] - 1s 915us/step - loss: 0.3153\n",
      "Epoch 57/200\n",
      "1000/1000 [==============================] - 1s 921us/step - loss: 0.3472\n",
      "Epoch 58/200\n",
      "1000/1000 [==============================] - 1s 918us/step - loss: 0.2878\n",
      "Epoch 59/200\n",
      "1000/1000 [==============================] - 1s 913us/step - loss: 0.3493\n",
      "Epoch 60/200\n",
      "1000/1000 [==============================] - 1s 915us/step - loss: 0.3441\n",
      "Epoch 61/200\n",
      "1000/1000 [==============================] - 1s 921us/step - loss: 0.2553\n",
      "Epoch 62/200\n",
      "1000/1000 [==============================] - 1s 920us/step - loss: 0.2611\n",
      "Epoch 63/200\n",
      "1000/1000 [==============================] - 1s 935us/step - loss: 0.2566\n",
      "Epoch 64/200\n",
      "1000/1000 [==============================] - 1s 966us/step - loss: 0.2916\n",
      "Epoch 65/200\n",
      "1000/1000 [==============================] - 1s 940us/step - loss: 0.3020\n",
      "Epoch 66/200\n",
      "1000/1000 [==============================] - 1s 922us/step - loss: 0.3734\n",
      "Epoch 67/200\n",
      "1000/1000 [==============================] - 1s 916us/step - loss: 0.3167\n",
      "Epoch 68/200\n",
      "1000/1000 [==============================] - 1s 920us/step - loss: 0.3216\n",
      "Epoch 69/200\n",
      "1000/1000 [==============================] - 1s 918us/step - loss: 0.2447\n",
      "Epoch 70/200\n",
      "1000/1000 [==============================] - 1s 917us/step - loss: 0.2799\n",
      "Epoch 71/200\n",
      "1000/1000 [==============================] - 1s 913us/step - loss: 0.2632\n",
      "Epoch 72/200\n",
      "1000/1000 [==============================] - 1s 915us/step - loss: 0.2421\n",
      "Epoch 73/200\n",
      "1000/1000 [==============================] - 1s 917us/step - loss: 0.2089\n",
      "Epoch 74/200\n",
      "1000/1000 [==============================] - 1s 932us/step - loss: 0.1780\n",
      "Epoch 75/200\n",
      "1000/1000 [==============================] - 1s 934us/step - loss: 0.2186\n",
      "Epoch 76/200\n",
      "1000/1000 [==============================] - 1s 934us/step - loss: 0.2252\n",
      "Epoch 77/200\n",
      "1000/1000 [==============================] - 1s 931us/step - loss: 0.2129\n",
      "Epoch 78/200\n",
      "1000/1000 [==============================] - 1s 918us/step - loss: 0.1583\n",
      "Epoch 79/200\n",
      "1000/1000 [==============================] - 1s 915us/step - loss: 0.1692\n",
      "Epoch 80/200\n",
      "1000/1000 [==============================] - 1s 925us/step - loss: 0.2048\n",
      "Epoch 81/200\n",
      "1000/1000 [==============================] - 1s 920us/step - loss: 0.2822\n",
      "Epoch 82/200\n",
      "1000/1000 [==============================] - 1s 917us/step - loss: 0.1960\n",
      "Epoch 83/200\n",
      "1000/1000 [==============================] - 1s 917us/step - loss: 0.1784\n",
      "Epoch 84/200\n",
      "1000/1000 [==============================] - 1s 915us/step - loss: 0.1714\n",
      "Epoch 85/200\n",
      "1000/1000 [==============================] - 1s 943us/step - loss: 0.1869\n",
      "Epoch 86/200\n",
      "1000/1000 [==============================] - 1s 936us/step - loss: 0.1778\n",
      "Epoch 87/200\n",
      "1000/1000 [==============================] - 1s 939us/step - loss: 0.1778\n",
      "Epoch 88/200\n",
      "1000/1000 [==============================] - 1s 945us/step - loss: 0.1865\n",
      "Epoch 89/200\n",
      "1000/1000 [==============================] - 1s 941us/step - loss: 0.1618\n",
      "Epoch 90/200\n",
      "1000/1000 [==============================] - 1s 953us/step - loss: 0.1378\n",
      "Epoch 91/200\n",
      "1000/1000 [==============================] - 1s 966us/step - loss: 0.1504\n",
      "Epoch 92/200\n",
      "1000/1000 [==============================] - 1s 986us/step - loss: 0.1558\n",
      "Epoch 93/200\n",
      "1000/1000 [==============================] - 1s 1000us/step - loss: 0.1440\n",
      "Epoch 94/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1232\n",
      "Epoch 95/200\n",
      "1000/1000 [==============================] - 1s 999us/step - loss: 0.1266\n",
      "Epoch 96/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2414\n",
      "Epoch 97/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2235\n",
      "Epoch 98/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1845\n",
      "Epoch 99/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1453\n",
      "Epoch 100/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1373\n",
      "Epoch 101/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1422\n",
      "Epoch 102/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1477\n",
      "Epoch 103/200\n",
      "1000/1000 [==============================] - 1s 999us/step - loss: 0.1887\n",
      "Epoch 104/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1534\n",
      "Epoch 105/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1177\n",
      "Epoch 106/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1628\n",
      "Epoch 107/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1405\n",
      "Epoch 108/200\n",
      "1000/1000 [==============================] - 1s 999us/step - loss: 0.1345\n",
      "Epoch 109/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1189\n",
      "Epoch 110/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1550\n",
      "Epoch 111/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1373\n",
      "Epoch 112/200\n",
      "1000/1000 [==============================] - 1s 996us/step - loss: 0.1419\n",
      "Epoch 113/200\n",
      "1000/1000 [==============================] - 1s 970us/step - loss: 0.1106\n",
      "Epoch 114/200\n",
      "1000/1000 [==============================] - 1s 978us/step - loss: 0.1352\n",
      "Epoch 115/200\n",
      "1000/1000 [==============================] - 1s 981us/step - loss: 0.1155\n",
      "Epoch 116/200\n",
      "1000/1000 [==============================] - 1s 978us/step - loss: 0.1151\n",
      "Epoch 117/200\n",
      "1000/1000 [==============================] - 1s 971us/step - loss: 0.1088\n",
      "Epoch 118/200\n",
      "1000/1000 [==============================] - 1s 973us/step - loss: 0.0922\n",
      "Epoch 119/200\n",
      "1000/1000 [==============================] - 1s 969us/step - loss: 0.0800\n",
      "Epoch 120/200\n",
      "1000/1000 [==============================] - 1s 970us/step - loss: 0.0908\n",
      "Epoch 121/200\n",
      "1000/1000 [==============================] - 1s 969us/step - loss: 0.0947\n",
      "Epoch 122/200\n",
      "1000/1000 [==============================] - 1s 966us/step - loss: 0.1114\n",
      "Epoch 123/200\n",
      "1000/1000 [==============================] - 1s 963us/step - loss: 0.1175\n",
      "Epoch 124/200\n",
      "1000/1000 [==============================] - 1s 989us/step - loss: 0.1030\n",
      "Epoch 125/200\n",
      "1000/1000 [==============================] - 1s 972us/step - loss: 0.1111\n",
      "Epoch 126/200\n",
      "1000/1000 [==============================] - 1s 974us/step - loss: 0.0952\n",
      "Epoch 127/200\n",
      "1000/1000 [==============================] - 1s 972us/step - loss: 0.1058\n",
      "Epoch 128/200\n",
      "1000/1000 [==============================] - 1s 975us/step - loss: 0.0977\n",
      "Epoch 129/200\n",
      "1000/1000 [==============================] - 1s 968us/step - loss: 0.1466\n",
      "Epoch 130/200\n",
      "1000/1000 [==============================] - 1s 968us/step - loss: 0.1632\n",
      "Epoch 131/200\n",
      "1000/1000 [==============================] - 1s 974us/step - loss: 0.1490\n",
      "Epoch 132/200\n",
      "1000/1000 [==============================] - 1s 985us/step - loss: 0.1773\n",
      "Epoch 133/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2548\n",
      "Epoch 134/200\n",
      "1000/1000 [==============================] - 1s 1000us/step - loss: 0.2890\n",
      "Epoch 135/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2738\n",
      "Epoch 136/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2442\n",
      "Epoch 137/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.3202\n",
      "Epoch 138/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2373\n",
      "Epoch 139/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2316\n",
      "Epoch 140/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1778\n",
      "Epoch 141/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1719\n",
      "Epoch 142/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1155\n",
      "Epoch 143/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1280\n",
      "Epoch 144/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1103\n",
      "Epoch 145/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1097\n",
      "Epoch 146/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1101\n",
      "Epoch 147/200\n",
      "1000/1000 [==============================] - 1s 972us/step - loss: 0.0894\n",
      "Epoch 148/200\n",
      "1000/1000 [==============================] - 1s 967us/step - loss: 0.0942\n",
      "Epoch 149/200\n",
      "1000/1000 [==============================] - 1s 963us/step - loss: 0.0981\n",
      "Epoch 150/200\n",
      "1000/1000 [==============================] - 1s 968us/step - loss: 0.1043\n",
      "Epoch 151/200\n",
      "1000/1000 [==============================] - 1s 969us/step - loss: 0.1377\n",
      "Epoch 152/200\n",
      "1000/1000 [==============================] - 1s 970us/step - loss: 0.1069\n",
      "Epoch 153/200\n",
      "1000/1000 [==============================] - 1s 966us/step - loss: 0.0775\n",
      "Epoch 154/200\n",
      "1000/1000 [==============================] - 1s 968us/step - loss: 0.0660\n",
      "Epoch 155/200\n",
      "1000/1000 [==============================] - 1s 963us/step - loss: 0.0622\n",
      "Epoch 156/200\n",
      "1000/1000 [==============================] - 1s 974us/step - loss: 0.0621\n",
      "Epoch 157/200\n",
      "1000/1000 [==============================] - 1s 973us/step - loss: 0.0648\n",
      "Epoch 158/200\n",
      "1000/1000 [==============================] - 1s 967us/step - loss: 0.0576\n",
      "Epoch 159/200\n",
      "1000/1000 [==============================] - 1s 974us/step - loss: 0.0734\n",
      "Epoch 160/200\n",
      "1000/1000 [==============================] - 1s 969us/step - loss: 0.0792\n",
      "Epoch 161/200\n",
      "1000/1000 [==============================] - 1s 968us/step - loss: 0.0836\n",
      "Epoch 162/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0630\n",
      "Epoch 163/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0655\n",
      "Epoch 164/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0532\n",
      "Epoch 165/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0683\n",
      "Epoch 166/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0553\n",
      "Epoch 167/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0596\n",
      "Epoch 168/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0598\n",
      "Epoch 169/200\n",
      "1000/1000 [==============================] - 1s 999us/step - loss: 0.0647\n",
      "Epoch 170/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0750\n",
      "Epoch 171/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0687\n",
      "Epoch 172/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0859\n",
      "Epoch 173/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1493\n",
      "Epoch 174/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2346\n",
      "Epoch 175/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1953\n",
      "Epoch 176/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1701\n",
      "Epoch 177/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1569\n",
      "Epoch 178/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1369\n",
      "Epoch 179/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1144\n",
      "Epoch 180/200\n",
      "1000/1000 [==============================] - 1s 972us/step - loss: 0.1226\n",
      "Epoch 181/200\n",
      "1000/1000 [==============================] - 1s 977us/step - loss: 0.1182\n",
      "Epoch 182/200\n",
      "1000/1000 [==============================] - 1s 974us/step - loss: 0.0737\n",
      "Epoch 183/200\n",
      "1000/1000 [==============================] - 1s 969us/step - loss: 0.0844\n",
      "Epoch 184/200\n",
      "1000/1000 [==============================] - 1s 969us/step - loss: 0.0902\n",
      "Epoch 185/200\n",
      "1000/1000 [==============================] - 1s 971us/step - loss: 0.0742\n",
      "Epoch 186/200\n",
      "1000/1000 [==============================] - 1s 980us/step - loss: 0.0702\n",
      "Epoch 187/200\n",
      "1000/1000 [==============================] - 1s 974us/step - loss: 0.0684\n",
      "Epoch 188/200\n",
      "1000/1000 [==============================] - 1s 967us/step - loss: 0.0569\n",
      "Epoch 189/200\n",
      "1000/1000 [==============================] - 1s 979us/step - loss: 0.0595\n",
      "Epoch 190/200\n",
      "1000/1000 [==============================] - 1s 970us/step - loss: 0.0543\n",
      "Epoch 191/200\n",
      "1000/1000 [==============================] - 1s 977us/step - loss: 0.0884\n",
      "Epoch 192/200\n",
      "1000/1000 [==============================] - 1s 982us/step - loss: 0.0790\n",
      "Epoch 193/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0651\n",
      "Epoch 194/200\n",
      "1000/1000 [==============================] - 1s 998us/step - loss: 0.0615\n",
      "Epoch 195/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0552\n",
      "Epoch 196/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0700\n",
      "Epoch 197/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0976\n",
      "Epoch 198/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0783\n",
      "Epoch 199/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0732\n",
      "Epoch 200/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0783\n",
      "--calculating error--\n",
      "Copying immediate reward for Ep: 0\n",
      "Copying immediate reward for Ep: 100\n",
      "Copying immediate reward for Ep: 200\n",
      "Copying immediate reward for Ep: 300\n",
      "Copying immediate reward for Ep: 400\n",
      "Copying immediate reward for Ep: 500\n",
      "Copying immediate reward for Ep: 600\n",
      "Copying immediate reward for Ep: 700\n",
      "Copying immediate reward for Ep: 800\n",
      "Copying immediate reward for Ep: 900\n",
      "MAE Error Delayed: 0.23618871278669662\n",
      "MAE Error Immediate: 0.29041932969730616\n",
      "--running 24--\n",
      "Epoch 1/200\n",
      "1000/1000 [==============================] - 15s 15ms/step - loss: 7.3580\n",
      "Epoch 2/200\n",
      "1000/1000 [==============================] - 1s 882us/step - loss: 1.6084\n",
      "Epoch 3/200\n",
      "1000/1000 [==============================] - 1s 881us/step - loss: 1.3695\n",
      "Epoch 4/200\n",
      "1000/1000 [==============================] - 1s 895us/step - loss: 1.3458\n",
      "Epoch 5/200\n",
      "1000/1000 [==============================] - 1s 903us/step - loss: 1.2169\n",
      "Epoch 6/200\n",
      "1000/1000 [==============================] - 1s 908us/step - loss: 1.1386\n",
      "Epoch 7/200\n",
      "1000/1000 [==============================] - 1s 912us/step - loss: 1.1171\n",
      "Epoch 8/200\n",
      "1000/1000 [==============================] - 1s 938us/step - loss: 1.2137\n",
      "Epoch 9/200\n",
      "1000/1000 [==============================] - 1s 962us/step - loss: 1.1295\n",
      "Epoch 10/200\n",
      "1000/1000 [==============================] - 1s 961us/step - loss: 1.0994\n",
      "Epoch 11/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 1.2250\n",
      "Epoch 12/200\n",
      "1000/1000 [==============================] - 1s 999us/step - loss: 1.3127\n",
      "Epoch 13/200\n",
      "1000/1000 [==============================] - 1s 976us/step - loss: 1.0535\n",
      "Epoch 14/200\n",
      "1000/1000 [==============================] - 1s 953us/step - loss: 1.0203\n",
      "Epoch 15/200\n",
      "1000/1000 [==============================] - 1s 942us/step - loss: 1.0281\n",
      "Epoch 16/200\n",
      "1000/1000 [==============================] - 1s 933us/step - loss: 0.9630\n",
      "Epoch 17/200\n",
      "1000/1000 [==============================] - 1s 941us/step - loss: 0.9306\n",
      "Epoch 18/200\n",
      "1000/1000 [==============================] - 1s 956us/step - loss: 0.8094\n",
      "Epoch 19/200\n",
      "1000/1000 [==============================] - 1s 969us/step - loss: 0.8273\n",
      "Epoch 20/200\n",
      "1000/1000 [==============================] - 1s 962us/step - loss: 0.7604\n",
      "Epoch 21/200\n",
      "1000/1000 [==============================] - 1s 954us/step - loss: 0.8805\n",
      "Epoch 22/200\n",
      "1000/1000 [==============================] - 1s 964us/step - loss: 0.9401\n",
      "Epoch 23/200\n",
      "1000/1000 [==============================] - 1s 964us/step - loss: 0.8106\n",
      "Epoch 24/200\n",
      "1000/1000 [==============================] - 1s 956us/step - loss: 0.7460\n",
      "Epoch 25/200\n",
      "1000/1000 [==============================] - 1s 961us/step - loss: 0.7277\n",
      "Epoch 26/200\n",
      "1000/1000 [==============================] - 1s 960us/step - loss: 0.6751\n",
      "Epoch 27/200\n",
      "1000/1000 [==============================] - 1s 940us/step - loss: 0.6810\n",
      "Epoch 28/200\n",
      "1000/1000 [==============================] - 1s 936us/step - loss: 0.7382\n",
      "Epoch 29/200\n",
      "1000/1000 [==============================] - 1s 963us/step - loss: 0.6709\n",
      "Epoch 30/200\n",
      "1000/1000 [==============================] - 1s 966us/step - loss: 0.6946\n",
      "Epoch 31/200\n",
      "1000/1000 [==============================] - 1s 960us/step - loss: 0.7308\n",
      "Epoch 32/200\n",
      "1000/1000 [==============================] - 1s 957us/step - loss: 0.6547\n",
      "Epoch 33/200\n",
      "1000/1000 [==============================] - 1s 959us/step - loss: 0.6398\n",
      "Epoch 34/200\n",
      "1000/1000 [==============================] - 1s 957us/step - loss: 0.5957\n",
      "Epoch 35/200\n",
      "1000/1000 [==============================] - 1s 964us/step - loss: 0.6155\n",
      "Epoch 36/200\n",
      "1000/1000 [==============================] - 1s 966us/step - loss: 0.5953\n",
      "Epoch 37/200\n",
      "1000/1000 [==============================] - 1s 962us/step - loss: 0.6521\n",
      "Epoch 38/200\n",
      "1000/1000 [==============================] - 1s 963us/step - loss: 0.6999\n",
      "Epoch 39/200\n",
      "1000/1000 [==============================] - 1s 962us/step - loss: 0.6367\n",
      "Epoch 40/200\n",
      "1000/1000 [==============================] - 1s 968us/step - loss: 0.5338\n",
      "Epoch 41/200\n",
      "1000/1000 [==============================] - 1s 961us/step - loss: 0.5214\n",
      "Epoch 42/200\n",
      "1000/1000 [==============================] - 1s 964us/step - loss: 0.4922\n",
      "Epoch 43/200\n",
      "1000/1000 [==============================] - 1s 954us/step - loss: 0.4405\n",
      "Epoch 44/200\n",
      "1000/1000 [==============================] - 1s 992us/step - loss: 0.4322\n",
      "Epoch 45/200\n",
      "1000/1000 [==============================] - 1s 963us/step - loss: 0.4828\n",
      "Epoch 46/200\n",
      "1000/1000 [==============================] - 1s 961us/step - loss: 0.6081\n",
      "Epoch 47/200\n",
      "1000/1000 [==============================] - 1s 957us/step - loss: 0.5417\n",
      "Epoch 48/200\n",
      "1000/1000 [==============================] - 1s 958us/step - loss: 0.4593\n",
      "Epoch 49/200\n",
      "1000/1000 [==============================] - 1s 965us/step - loss: 0.4781\n",
      "Epoch 50/200\n",
      "1000/1000 [==============================] - 1s 967us/step - loss: 0.4130\n",
      "Epoch 51/200\n",
      "1000/1000 [==============================] - 1s 961us/step - loss: 0.3691\n",
      "Epoch 52/200\n",
      "1000/1000 [==============================] - 1s 957us/step - loss: 0.3582\n",
      "Epoch 53/200\n",
      "1000/1000 [==============================] - 1s 958us/step - loss: 0.4078\n",
      "Epoch 54/200\n",
      "1000/1000 [==============================] - 1s 961us/step - loss: 0.4336\n",
      "Epoch 55/200\n",
      "1000/1000 [==============================] - 1s 962us/step - loss: 0.4465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/200\n",
      "1000/1000 [==============================] - 1s 962us/step - loss: 0.4468\n",
      "Epoch 57/200\n",
      "1000/1000 [==============================] - 1s 964us/step - loss: 0.5099\n",
      "Epoch 58/200\n",
      "1000/1000 [==============================] - 1s 963us/step - loss: 0.4163\n",
      "Epoch 59/200\n",
      "1000/1000 [==============================] - 1s 963us/step - loss: 0.4852\n",
      "Epoch 60/200\n",
      "1000/1000 [==============================] - 1s 965us/step - loss: 0.3999\n",
      "Epoch 61/200\n",
      "1000/1000 [==============================] - 1s 962us/step - loss: 0.3161\n",
      "Epoch 62/200\n",
      "1000/1000 [==============================] - 1s 963us/step - loss: 0.3175\n",
      "Epoch 63/200\n",
      "1000/1000 [==============================] - 1s 956us/step - loss: 0.3159\n",
      "Epoch 64/200\n",
      "1000/1000 [==============================] - 1s 958us/step - loss: 0.4051\n",
      "Epoch 65/200\n",
      "1000/1000 [==============================] - 1s 962us/step - loss: 0.3638\n",
      "Epoch 66/200\n",
      "1000/1000 [==============================] - 1s 967us/step - loss: 0.3972\n",
      "Epoch 67/200\n",
      "1000/1000 [==============================] - 1s 966us/step - loss: 0.3576\n",
      "Epoch 68/200\n",
      "1000/1000 [==============================] - 1s 963us/step - loss: 0.3300\n",
      "Epoch 69/200\n",
      "1000/1000 [==============================] - 1s 958us/step - loss: 0.2799\n",
      "Epoch 70/200\n",
      "1000/1000 [==============================] - 1s 963us/step - loss: 0.3184\n",
      "Epoch 71/200\n",
      "1000/1000 [==============================] - 1s 976us/step - loss: 0.2733\n",
      "Epoch 72/200\n",
      "1000/1000 [==============================] - 1s 961us/step - loss: 0.2447\n",
      "Epoch 73/200\n",
      "1000/1000 [==============================] - 1s 956us/step - loss: 0.2735\n",
      "Epoch 74/200\n",
      "1000/1000 [==============================] - 1s 964us/step - loss: 0.2124\n",
      "Epoch 75/200\n",
      "1000/1000 [==============================] - 1s 980us/step - loss: 0.2250\n",
      "Epoch 76/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2185\n",
      "Epoch 77/200\n",
      "1000/1000 [==============================] - 1s 986us/step - loss: 0.2169\n",
      "Epoch 78/200\n",
      "1000/1000 [==============================] - 1s 964us/step - loss: 0.2151\n",
      "Epoch 79/200\n",
      "1000/1000 [==============================] - 1s 959us/step - loss: 0.2089\n",
      "Epoch 80/200\n",
      "1000/1000 [==============================] - 1s 964us/step - loss: 0.2091\n",
      "Epoch 81/200\n",
      "1000/1000 [==============================] - 1s 961us/step - loss: 0.1914\n",
      "Epoch 82/200\n",
      "1000/1000 [==============================] - 1s 961us/step - loss: 0.1553\n",
      "Epoch 83/200\n",
      "1000/1000 [==============================] - 1s 958us/step - loss: 0.1579\n",
      "Epoch 84/200\n",
      "1000/1000 [==============================] - 1s 960us/step - loss: 0.1624\n",
      "Epoch 85/200\n",
      "1000/1000 [==============================] - 1s 960us/step - loss: 0.1733\n",
      "Epoch 86/200\n",
      "1000/1000 [==============================] - 1s 968us/step - loss: 0.1510\n",
      "Epoch 87/200\n",
      "1000/1000 [==============================] - 1s 963us/step - loss: 0.1410\n",
      "Epoch 88/200\n",
      "1000/1000 [==============================] - 1s 974us/step - loss: 0.1566\n",
      "Epoch 89/200\n",
      "1000/1000 [==============================] - 1s 981us/step - loss: 0.1482\n",
      "Epoch 90/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1830\n",
      "Epoch 91/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2147\n",
      "Epoch 92/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2172\n",
      "Epoch 93/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1995\n",
      "Epoch 94/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1450\n",
      "Epoch 95/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1419\n",
      "Epoch 96/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1350\n",
      "Epoch 97/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1480\n",
      "Epoch 98/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1448\n",
      "Epoch 99/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2300\n",
      "Epoch 100/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2151\n",
      "Epoch 101/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1912\n",
      "Epoch 102/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1518\n",
      "Epoch 103/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1455\n",
      "Epoch 104/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1187\n",
      "Epoch 105/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0985\n",
      "Epoch 106/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1094\n",
      "Epoch 107/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1271\n",
      "Epoch 108/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1088\n",
      "Epoch 109/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1012\n",
      "Epoch 110/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1156\n",
      "Epoch 111/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1335\n",
      "Epoch 112/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1225\n",
      "Epoch 113/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1019\n",
      "Epoch 114/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1475\n",
      "Epoch 115/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1211\n",
      "Epoch 116/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1125\n",
      "Epoch 117/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1322\n",
      "Epoch 118/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1399\n",
      "Epoch 119/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1184\n",
      "Epoch 120/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1063\n",
      "Epoch 121/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0898\n",
      "Epoch 122/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0754\n",
      "Epoch 123/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0781\n",
      "Epoch 124/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0790\n",
      "Epoch 125/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0880\n",
      "Epoch 126/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1001\n",
      "Epoch 127/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1023\n",
      "Epoch 128/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1353\n",
      "Epoch 129/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1775\n",
      "Epoch 130/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2182\n",
      "Epoch 131/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2341\n",
      "Epoch 132/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2767\n",
      "Epoch 133/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2850\n",
      "Epoch 134/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2848\n",
      "Epoch 135/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2755\n",
      "Epoch 136/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2461\n",
      "Epoch 137/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1615\n",
      "Epoch 138/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1515\n",
      "Epoch 139/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1216\n",
      "Epoch 140/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1101\n",
      "Epoch 141/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0904\n",
      "Epoch 142/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0877\n",
      "Epoch 143/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1031\n",
      "Epoch 144/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1645\n",
      "Epoch 145/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2603\n",
      "Epoch 146/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2145\n",
      "Epoch 147/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1245\n",
      "Epoch 148/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1345\n",
      "Epoch 149/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1014\n",
      "Epoch 150/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0849\n",
      "Epoch 151/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0695\n",
      "Epoch 152/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0597\n",
      "Epoch 153/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0731\n",
      "Epoch 154/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0717\n",
      "Epoch 155/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0829\n",
      "Epoch 156/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0814\n",
      "Epoch 157/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0800\n",
      "Epoch 158/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0892\n",
      "Epoch 159/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0956\n",
      "Epoch 160/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1341\n",
      "Epoch 161/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1414\n",
      "Epoch 162/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1257\n",
      "Epoch 163/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0959\n",
      "Epoch 164/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0824\n",
      "Epoch 165/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1079\n",
      "Epoch 166/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0887\n",
      "Epoch 167/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1047\n",
      "Epoch 168/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1040\n",
      "Epoch 169/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0815\n",
      "Epoch 170/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1368\n",
      "Epoch 171/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2333\n",
      "Epoch 172/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.3231\n",
      "Epoch 173/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.4257\n",
      "Epoch 174/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2394\n",
      "Epoch 175/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2196\n",
      "Epoch 176/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2139\n",
      "Epoch 177/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1420\n",
      "Epoch 178/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1217\n",
      "Epoch 179/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1351\n",
      "Epoch 180/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0957\n",
      "Epoch 181/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0746\n",
      "Epoch 182/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0843\n",
      "Epoch 183/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0583\n",
      "Epoch 184/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0436\n",
      "Epoch 185/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0405\n",
      "Epoch 186/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0394\n",
      "Epoch 187/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0330\n",
      "Epoch 188/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0357A: 0s\n",
      "Epoch 189/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0505\n",
      "Epoch 190/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0555\n",
      "Epoch 191/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0574\n",
      "Epoch 192/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0737\n",
      "Epoch 193/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0770\n",
      "Epoch 194/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0838\n",
      "Epoch 195/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0635\n",
      "Epoch 196/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0578\n",
      "Epoch 197/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0577\n",
      "Epoch 198/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0450\n",
      "Epoch 199/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0363\n",
      "Epoch 200/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0443\n",
      "--calculating error--\n",
      "Copying immediate reward for Ep: 0\n",
      "Copying immediate reward for Ep: 100\n",
      "Copying immediate reward for Ep: 200\n",
      "Copying immediate reward for Ep: 300\n",
      "Copying immediate reward for Ep: 400\n",
      "Copying immediate reward for Ep: 500\n",
      "Copying immediate reward for Ep: 600\n",
      "Copying immediate reward for Ep: 700\n",
      "Copying immediate reward for Ep: 800\n",
      "Copying immediate reward for Ep: 900\n",
      "MAE Error Delayed: 0.2515983895588666\n",
      "MAE Error Immediate: 0.2850146823489081\n",
      "--running 32--\n",
      "Epoch 1/200\n",
      "1000/1000 [==============================] - 17s 17ms/step - loss: 15.0059\n",
      "Epoch 2/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 1.7568\n",
      "Epoch 3/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 1.3820\n",
      "Epoch 4/200\n",
      "1000/1000 [==============================] - 1s 988us/step - loss: 1.3548\n",
      "Epoch 5/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 1.2628\n",
      "Epoch 6/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 1.1948\n",
      "Epoch 7/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 1.1505\n",
      "Epoch 8/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 1.1937\n",
      "Epoch 9/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 1.1660\n",
      "Epoch 10/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 1.2238\n",
      "Epoch 11/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 1.0822\n",
      "Epoch 12/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 1.1262\n",
      "Epoch 13/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 1.0168\n",
      "Epoch 14/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 1.0993\n",
      "Epoch 15/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 1.0308\n",
      "Epoch 16/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 1.0394\n",
      "Epoch 17/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.8766\n",
      "Epoch 18/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 1.0953\n",
      "Epoch 19/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.9850\n",
      "Epoch 20/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.9182\n",
      "Epoch 21/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 1.0211\n",
      "Epoch 22/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.9797\n",
      "Epoch 23/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.7931\n",
      "Epoch 24/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.7664\n",
      "Epoch 25/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.7848\n",
      "Epoch 26/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.8012\n",
      "Epoch 27/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.8753\n",
      "Epoch 28/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6985\n",
      "Epoch 29/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.7128\n",
      "Epoch 30/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6700\n",
      "Epoch 31/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6193\n",
      "Epoch 32/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6571\n",
      "Epoch 33/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.5813\n",
      "Epoch 34/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.7552\n",
      "Epoch 35/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6939\n",
      "Epoch 36/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.5732\n",
      "Epoch 37/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.5652\n",
      "Epoch 38/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.5556\n",
      "Epoch 39/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.5261\n",
      "Epoch 40/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.5841\n",
      "Epoch 41/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.5930\n",
      "Epoch 42/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6445\n",
      "Epoch 43/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.4927\n",
      "Epoch 44/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.5464\n",
      "Epoch 45/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.5770\n",
      "Epoch 46/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.4693\n",
      "Epoch 47/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.4717\n",
      "Epoch 48/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.5269\n",
      "Epoch 49/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.4810\n",
      "Epoch 50/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.5201\n",
      "Epoch 51/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.4302\n",
      "Epoch 52/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.4192\n",
      "Epoch 53/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.5457\n",
      "Epoch 54/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.4340\n",
      "Epoch 55/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.3576\n",
      "Epoch 56/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.3961\n",
      "Epoch 57/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.3852\n",
      "Epoch 58/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.3775\n",
      "Epoch 59/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.3415\n",
      "Epoch 60/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.3142\n",
      "Epoch 61/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2844\n",
      "Epoch 62/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2692\n",
      "Epoch 63/200\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.2969\n",
      "Epoch 64/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.3213\n",
      "Epoch 65/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.3401\n",
      "Epoch 66/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.3444\n",
      "Epoch 67/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.3124\n",
      "Epoch 68/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.3571\n",
      "Epoch 69/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.3326\n",
      "Epoch 70/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2711\n",
      "Epoch 71/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2770\n",
      "Epoch 72/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2696\n",
      "Epoch 73/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2621\n",
      "Epoch 74/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2082\n",
      "Epoch 75/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1780\n",
      "Epoch 76/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2091\n",
      "Epoch 77/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1985\n",
      "Epoch 78/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1713\n",
      "Epoch 79/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1904\n",
      "Epoch 80/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1765\n",
      "Epoch 81/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1533\n",
      "Epoch 82/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2683\n",
      "Epoch 83/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2121\n",
      "Epoch 84/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1884\n",
      "Epoch 85/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1670\n",
      "Epoch 86/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2424\n",
      "Epoch 87/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2347\n",
      "Epoch 88/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2252\n",
      "Epoch 89/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1987\n",
      "Epoch 90/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1708\n",
      "Epoch 91/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1787\n",
      "Epoch 92/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1519\n",
      "Epoch 93/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1489\n",
      "Epoch 94/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1324\n",
      "Epoch 95/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1075\n",
      "Epoch 96/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1358\n",
      "Epoch 97/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1838\n",
      "Epoch 98/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1804\n",
      "Epoch 99/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1829\n",
      "Epoch 100/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1406\n",
      "Epoch 101/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1356\n",
      "Epoch 102/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1185\n",
      "Epoch 103/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1322\n",
      "Epoch 104/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1456\n",
      "Epoch 105/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1405\n",
      "Epoch 106/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1408\n",
      "Epoch 107/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1931\n",
      "Epoch 108/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1449\n",
      "Epoch 109/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1224\n",
      "Epoch 110/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1632\n",
      "Epoch 111/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1376\n",
      "Epoch 112/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1529\n",
      "Epoch 113/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1380\n",
      "Epoch 114/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1259\n",
      "Epoch 115/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1528\n",
      "Epoch 116/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2017\n",
      "Epoch 117/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1277\n",
      "Epoch 118/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1423\n",
      "Epoch 119/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1715\n",
      "Epoch 120/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2011\n",
      "Epoch 121/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2560\n",
      "Epoch 122/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2082\n",
      "Epoch 123/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2322\n",
      "Epoch 124/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1876\n",
      "Epoch 125/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1366\n",
      "Epoch 126/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1929\n",
      "Epoch 127/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1659\n",
      "Epoch 128/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1613\n",
      "Epoch 129/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1721\n",
      "Epoch 130/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1310\n",
      "Epoch 131/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1155\n",
      "Epoch 132/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1177\n",
      "Epoch 133/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1014\n",
      "Epoch 134/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0969\n",
      "Epoch 135/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0889\n",
      "Epoch 136/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0914\n",
      "Epoch 137/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0934\n",
      "Epoch 138/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0741\n",
      "Epoch 139/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0690\n",
      "Epoch 140/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0654\n",
      "Epoch 141/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0645\n",
      "Epoch 142/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0655\n",
      "Epoch 143/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0574\n",
      "Epoch 144/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0551\n",
      "Epoch 145/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0534\n",
      "Epoch 146/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0806\n",
      "Epoch 147/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0924\n",
      "Epoch 148/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0919\n",
      "Epoch 149/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0742\n",
      "Epoch 150/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0689\n",
      "Epoch 151/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0671\n",
      "Epoch 152/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0814\n",
      "Epoch 153/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1168\n",
      "Epoch 154/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1220\n",
      "Epoch 155/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1078\n",
      "Epoch 156/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0891\n",
      "Epoch 157/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1299\n",
      "Epoch 158/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1891\n",
      "Epoch 159/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.3746\n",
      "Epoch 160/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.4824\n",
      "Epoch 161/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.6610\n",
      "Epoch 162/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.4848\n",
      "Epoch 163/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.3292\n",
      "Epoch 164/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2855\n",
      "Epoch 165/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.3606\n",
      "Epoch 166/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2874\n",
      "Epoch 167/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2586\n",
      "Epoch 168/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.2018\n",
      "Epoch 169/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1492\n",
      "Epoch 170/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1291\n",
      "Epoch 171/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1023\n",
      "Epoch 172/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1147\n",
      "Epoch 173/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0810\n",
      "Epoch 174/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0693\n",
      "Epoch 175/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0618\n",
      "Epoch 176/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0491\n",
      "Epoch 177/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0471\n",
      "Epoch 178/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0408\n",
      "Epoch 179/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0324\n",
      "Epoch 180/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0475\n",
      "Epoch 181/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0600\n",
      "Epoch 182/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0540\n",
      "Epoch 183/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0694\n",
      "Epoch 184/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0772\n",
      "Epoch 185/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1294\n",
      "Epoch 186/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1070\n",
      "Epoch 187/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1029\n",
      "Epoch 188/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0793\n",
      "Epoch 189/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1010\n",
      "Epoch 190/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0744\n",
      "Epoch 191/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0594\n",
      "Epoch 192/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0550\n",
      "Epoch 193/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0716\n",
      "Epoch 194/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1049\n",
      "Epoch 195/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1314\n",
      "Epoch 196/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1112\n",
      "Epoch 197/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0724\n",
      "Epoch 198/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0612\n",
      "Epoch 199/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0446\n",
      "Epoch 200/200\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0721\n",
      "--calculating error--\n",
      "Copying immediate reward for Ep: 0\n",
      "Copying immediate reward for Ep: 100\n",
      "Copying immediate reward for Ep: 200\n",
      "Copying immediate reward for Ep: 300\n",
      "Copying immediate reward for Ep: 400\n",
      "Copying immediate reward for Ep: 500\n",
      "Copying immediate reward for Ep: 600\n",
      "Copying immediate reward for Ep: 700\n",
      "Copying immediate reward for Ep: 800\n",
      "Copying immediate reward for Ep: 900\n",
      "MAE Error Delayed: 0.3631224665498357\n",
      "MAE Error Immediate: 0.25569251421985495\n",
      "Best Units: 8 | MAE: 0.22344178561456454\n"
     ]
    }
   ],
   "source": [
    "unit, err, df = run_and_optimize(X, y, df.copy())\n",
    "print(\"Best Units:\", unit, \"| MAE:\", err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T22:34:39.401150Z",
     "start_time": "2020-11-05T22:34:39.397993Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build a model\n",
    "# model = get_model(state_size=feature_length, history=history, unit=16)\n",
    "# keras.utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T22:34:39.405035Z",
     "start_time": "2020-11-05T22:34:39.402983Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model.fit(X,y, epochs=500)\n",
    "\n",
    "# # This is the infer reward\n",
    "# f = keras.backend.function(model.input, model.layers[-1].input) \n",
    "# infer_reward = f([X,1])\n",
    "\n",
    "# pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T22:34:39.409135Z",
     "start_time": "2020-11-05T22:34:39.407062Z"
    }
   },
   "outputs": [],
   "source": [
    "# df['infer_reward'] = 0\n",
    "# for ep in sorted(df['episode_id'].unique()):\n",
    "#     ep_total = 0\n",
    "#     if ep%100==0:\n",
    "#         print(\"Running Ep:\", ep)\n",
    "        \n",
    "#     curr_trans = max(df.loc[df['episode_id']==ep, 'transition_id'])\n",
    "#     # this loops from last transitions to back\n",
    "#     for h in reversed(range(history)):\n",
    "#         immediate_reward = infer_reward[h][ep][0][0]\n",
    "#         ep_total += immediate_reward\n",
    "#         if curr_trans>=0:\n",
    "#             df.loc[(df['episode_id']==ep) & (df['transition_id']==curr_trans), 'infer_reward'] = immediate_reward\n",
    "#             curr_trans-=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T22:34:39.695635Z",
     "start_time": "2020-11-05T22:34:39.410961Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE Error Delayed: 0.22344178561456454\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "a = df.groupby(['episode_id']).sum()\n",
    "print(\"MAE Error Delayed:\", sum(abs(a['delayed_reward'] - a['infer_reward']))/len(a))\n",
    "# print(\"MAE Error Immediate:\", sum(abs(df['immediate_reward'] - df['infer_reward']))/len(df))\n",
    "a.describe()\n",
    "\n",
    "df.to_pickle(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GP Infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T22:34:40.284120Z",
     "start_time": "2020-11-05T22:34:39.697248Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_id</th>\n",
       "      <th>transition_id</th>\n",
       "      <th>state</th>\n",
       "      <th>action</th>\n",
       "      <th>immediate_reward</th>\n",
       "      <th>delayed_reward</th>\n",
       "      <th>infer_reward</th>\n",
       "      <th>infer_reward_gp</th>\n",
       "      <th>done</th>\n",
       "      <th>next_state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>[0, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 2]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>[1, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>[1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>[1, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35935</th>\n",
       "      <td>999</td>\n",
       "      <td>34</td>\n",
       "      <td>[13, 1]</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.029318</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>[13, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35936</th>\n",
       "      <td>999</td>\n",
       "      <td>35</td>\n",
       "      <td>[13, 2]</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.550923</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>[13, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35937</th>\n",
       "      <td>999</td>\n",
       "      <td>36</td>\n",
       "      <td>[13, 3]</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.047019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>[13, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35938</th>\n",
       "      <td>999</td>\n",
       "      <td>37</td>\n",
       "      <td>[13, 4]</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.038777</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>[13, 5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35939</th>\n",
       "      <td>999</td>\n",
       "      <td>38</td>\n",
       "      <td>[13, 5]</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-2.9</td>\n",
       "      <td>-0.689035</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>[13, 6]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35940 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      episode_id transition_id    state action  immediate_reward  \\\n",
       "0              0             0   [0, 0]      0              -0.1   \n",
       "1              0             1   [0, 1]      0              -0.1   \n",
       "2              0             2   [0, 2]      1               0.9   \n",
       "3              0             3   [1, 2]      2              -1.1   \n",
       "4              0             4   [1, 1]      2              -0.1   \n",
       "...          ...           ...      ...    ...               ...   \n",
       "35935        999            34  [13, 1]      0              -0.1   \n",
       "35936        999            35  [13, 2]      0              -0.1   \n",
       "35937        999            36  [13, 3]      0              -0.1   \n",
       "35938        999            37  [13, 4]      0              -0.1   \n",
       "35939        999            38  [13, 5]      0              -0.1   \n",
       "\n",
       "       delayed_reward  infer_reward  infer_reward_gp   done next_state  \n",
       "0                 0.0      0.000000              NaN  False     [0, 1]  \n",
       "1                 0.0      0.000000              NaN  False     [0, 2]  \n",
       "2                 0.0      0.000000              NaN  False     [1, 2]  \n",
       "3                 0.0      0.000000              NaN  False     [1, 1]  \n",
       "4                 0.0      0.000000              NaN  False     [1, 2]  \n",
       "...               ...           ...              ...    ...        ...  \n",
       "35935             0.0     -0.029318              NaN  False    [13, 2]  \n",
       "35936             0.0     -0.550923              NaN  False    [13, 3]  \n",
       "35937             0.0     -0.047019              NaN  False    [13, 4]  \n",
       "35938             0.0     -1.038777              NaN  False    [13, 5]  \n",
       "35939            -2.9     -0.689035              NaN   True    [13, 6]  \n",
       "\n",
       "[35940 rows x 10 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import delayed_reward_fun\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "\n",
    "random_state=0\n",
    "np.random.seed(random_state)\n",
    "random.seed(random_state)\n",
    "\n",
    "dataset = '../data/gridworldchi_ndm_slide_1k.pkl'\n",
    "\n",
    "df = pd.read_pickle(dataset)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T22:34:40.303839Z",
     "start_time": "2020-11-05T22:34:40.286317Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def _discounted_array(filled_till, x, size, gamma):\n",
    "    arr = []\n",
    "    val = 1\n",
    "    for i in range(size):\n",
    "        if i<filled_till:\n",
    "            arr.append(0)\n",
    "        elif i==filled_till:\n",
    "            arr.append(val)\n",
    "        elif i<filled_till+x:\n",
    "            val = val * gamma\n",
    "            arr.append(val)\n",
    "        else:\n",
    "            arr.append(0)\n",
    "    return arr, filled_till+x\n",
    "\n",
    "\n",
    "def _get_inferred_processed_df(df, mio_r_dif, cross_validation=True):\n",
    "    \"\"\"\n",
    "    This version uses original implementation from Dr Chi's lab.\n",
    "    \"\"\"\n",
    "    \n",
    "    feature_state = np.array(df['state'].tolist())\n",
    "    action = np.array(df['action'].tolist())\n",
    "    temp = np.array(df.loc[df['done']==True]['delayed_reward'].tolist())\n",
    "    delayed_reward = temp.reshape((len(temp), 1))\n",
    "\n",
    "    temp = np.array(df['episode_id'].tolist())\n",
    "    episode_index = temp.reshape((len(temp), 1))\n",
    "\n",
    "    #transfer features to feature action pairs\n",
    "    phi, phi_list = delayed_reward_fun.prepare_phi(feature_state, action)\n",
    "    \n",
    "    final_H_P_gamma_R = 0.5\n",
    "    final_H_P_sigma_R = 0.01\n",
    "    final_H_P_gamma = 1\n",
    "    if cross_validation:\n",
    "        H_P_discounted_reward_search = [1, 0.95, 0.9]\n",
    "        # rbf (gaussian) kernel variables K(x, y) = exp(-gamma ||x-y||^2)\n",
    "        H_P_gamma_search = [0.1, 0.3, 0.5, 0.7, 0.9, 1]\n",
    "        # Noise hyperparameter\n",
    "        H_P_sigma_search = [0.01, 0.05, 0.1]\n",
    "        # Error\n",
    "        GP_error = np.zeros((len(H_P_discounted_reward_search), len(H_P_gamma_search), len(H_P_sigma_search)))\n",
    "        min_error = 999999999999\n",
    "        \n",
    "        for discounted_reward_index, discounted_reward in enumerate(H_P_discounted_reward_search):\n",
    "            d = []\n",
    "            filled = 0\n",
    "            temp = df.groupby('episode_id').count()[['action']].reset_index()\n",
    "            for row in temp.iterrows():\n",
    "                x = row[1]['action']\n",
    "                arr, filled = _discounted_array(filled, x, len(feature_state), discounted_reward)\n",
    "                d.append(arr)\n",
    "            Dmat = np.matrix(d)\n",
    "            D = np.array(Dmat)                \n",
    "                            \n",
    "                            \n",
    "            for H_P_gamma_index, H_P_gamma_R in enumerate(H_P_gamma_search):\n",
    "                for H_P_sigma_index, H_P_sigma_R in enumerate(H_P_sigma_search):\n",
    "                    print(\"cross validation Hyperparameters are\", discounted_reward, H_P_gamma_R, H_P_sigma_R)\n",
    "                    t1 = time.time()\n",
    "                    GP_error[discounted_reward_index, H_P_gamma_index, H_P_sigma_index] = delayed_reward_fun.gaussian_process(phi, \n",
    "                                                                                                     D, episode_index, \n",
    "                                                                                                     delayed_reward, \n",
    "                                                                                                     'rbf', \n",
    "                                                                                                     H_P_gamma_R, \n",
    "                                                                                                     H_P_sigma_R, True,\n",
    "                                                                                                    mio_r_dif=mio_r_dif)\n",
    "\n",
    "                    current_error = GP_error[discounted_reward_index, H_P_gamma_index, H_P_sigma_index]\n",
    "                    if current_error < min_error:\n",
    "                        min_error = current_error\n",
    "                        final_H_P_gamma_R = H_P_gamma_R\n",
    "                        final_H_P_sigma_R = H_P_sigma_R\n",
    "                        final_H_P_gamma = discounted_reward\n",
    "\n",
    "                    print(\"sum of squared error is:\", GP_error[discounted_reward_index, H_P_gamma_index, H_P_sigma_index])\n",
    "                    print(\"training takes\", time.time()- t1 , \"seconds\")\n",
    "                    print(\"************************\")\n",
    "\n",
    "    print(\"Final:\", \"gamma:\", final_H_P_gamma, \"gamma_R:\", final_H_P_gamma_R, \"sigma_R:\", final_H_P_sigma_R, \"mse:\", min_error)\n",
    "    d = []\n",
    "    filled = 0\n",
    "    temp = df.groupby('episode_id').count()[['action']].reset_index()\n",
    "    for row in temp.iterrows():\n",
    "        x = row[1]['action']\n",
    "        arr, filled = _discounted_array(filled, x, len(feature_state), final_H_P_gamma)\n",
    "        d.append(arr)\n",
    "    Dmat = np.matrix(d)\n",
    "    D = np.array(Dmat) \n",
    "    # Expected value cov of inferred reward\n",
    "    E_r_R, C_r_R = delayed_reward_fun.gaussian_process(phi, D, episode_index, delayed_reward, 'rbf', \n",
    "                                                       final_H_P_gamma_R, final_H_P_sigma_R, False)\n",
    "    \n",
    "    return E_r_R, C_r_R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-05T22:04:52.118Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validation Hyperparameters are 1 0.1 0.01\n",
      "sum of squared error is: 458.40355243907777\n",
      "training takes 3056.6725368499756 seconds\n",
      "************************\n",
      "cross validation Hyperparameters are 1 0.1 0.05\n",
      "sum of squared error is: 470.6683273531572\n",
      "training takes 3689.5273818969727 seconds\n",
      "************************\n",
      "cross validation Hyperparameters are 1 0.1 0.1\n"
     ]
    }
   ],
   "source": [
    "E_r, C_r = _get_inferred_processed_df(df, mio_r_dif=0)\n",
    "\n",
    "df['infer_reward_gp'] = E_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-05T22:04:52.120Z"
    }
   },
   "outputs": [],
   "source": [
    "a = df.groupby(['episode_id']).sum()\n",
    "print(\"MAE Error Delayed:\", sum(abs(a['delayed_reward'] - a['infer_reward_gp']))/len(a))\n",
    "print(\"MAE Error Immediate:\", sum(abs(df['immediate_reward'] - df['infer_reward_gp']))/len(df))\n",
    "a.describe()\n",
    "\n",
    "df.to_pickle(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
